<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>phi.math API documentation</title>
<meta name="description" content="Vectorized operations, tensors with named dimensions …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>phi.math</code></h1>
</header>
<section id="section-intro">
<p>Vectorized operations, tensors with named dimensions.</p>
<p>This package provides a common interface for tensor operations.
Is internally uses NumPy, TensorFlow or PyTorch.</p>
<p>Provides</p>
<ul>
<li>A tensor base class with multiple implementations</li>
<li>A NumPy-like API for mathematical operations over tensors as well as tensor generation</li>
</ul>
<p>The provided operations are not implemented directly.
Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
This allows the user to write simulation code once and have it run with various computation backends.</p>
<p>Main classes:</p>
<ul>
<li>Tensor</li>
<li>Shape</li>
</ul>
<p>See the <code><a title="phi.math" href="#phi.math">phi.math</a></code> module documentation at <a href="https://tum-pbs.github.io/PhiFlow/Math.html">https://tum-pbs.github.io/PhiFlow/Math.html</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Vectorized operations, tensors with named dimensions.

This package provides a common interface for tensor operations.
Is internally uses NumPy, TensorFlow or PyTorch.

Provides

* A tensor base class with multiple implementations
* A NumPy-like API for mathematical operations over tensors as well as tensor generation

The provided operations are not implemented directly.
Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
This allows the user to write simulation code once and have it run with various computation backends.

Main classes:

* Tensor
* Shape

See the `phi.math` module documentation at https://tum-pbs.github.io/PhiFlow/Math.html
&#34;&#34;&#34;

from .backend import precision, set_global_precision, get_precision, Solve, LinearSolve, DType, choose_backend
from .backend._scipy_backend import SCIPY_BACKEND

from .extrapolation import Extrapolation

from ._config import GLOBAL_AXIS_ORDER

from ._shape import Shape, spatial_shape, EMPTY_SHAPE, batch_shape, channel_shape, shape
from ._tensors import tensor, tensors, Tensor
from ._functions import (
    all_available,
    print_ as print,
    transpose,
    zeros, ones, fftfreq, random_normal, random_uniform, meshgrid, linspace,  # creation operators (use default backend)
    batch_stack, spatial_stack, channel_stack, unstack, concat,
    pad, spatial_pad,
    join_dimensions,
    prod,
    divide_no_nan,
    where, nonzero,
    sum_ as sum, mean, std,
    zeros_like, ones_like,
    dot,
    matmul,
    einsum,
    abs,
    sign,
    round_ as round, ceil, floor,
    max_ as max, min_ as min, maximum, minimum, clip,
    with_custom_gradient,
    sqrt, exp, sin, cos,
    conv,
    to_float, to_int, to_complex, imag, real,
    boolean_mask,
    isfinite,
    closest_grid_values, grid_sample, scatter,
    any_ as any, all_ as all,
    fft, ifft,
    dtype, cast,
    tile, expand_channel,
    sparse_tensor,
    close, assert_close,
    solve,
)
from ._nd import (
    shift,
    spatial_sum, vec_abs, vec_squared, cross_product,
    normalize_to,
    l1_loss, l2_loss, l_n_loss, frequency_loss,
    gradient, laplace,
    fourier_laplace, fourier_poisson, abs_square,
    downsample2x, upsample2x, sample_subgrid,
)

PI = 3.14159265358979323846
&#34;&#34;&#34;Value of π to double precision &#34;&#34;&#34;
pi = PI

SCIPY_BACKEND = SCIPY_BACKEND  # to show up in pdoc
&#34;&#34;&#34;Default backend for NumPy arrays and SciPy objects.&#34;&#34;&#34;

__all__ = [key for key in globals().keys() if not key.startswith(&#39;_&#39;)]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code></dt>
<dd>
<div class="desc"><p>Low-level library wrappers for delegating vector operations.</p></div>
</dd>
<dt><code class="name"><a title="phi.math.extrapolation" href="extrapolation.html">phi.math.extrapolation</a></code></dt>
<dd>
<div class="desc"><p>Defines standard extrapolations …</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="phi.math.PI"><code class="name">var <span class="ident">PI</span></code></dt>
<dd>
<div class="desc"><p>Value of π to double precision</p></div>
</dd>
<dt id="phi.math.SCIPY_BACKEND"><code class="name">var <span class="ident">SCIPY_BACKEND</span></code></dt>
<dd>
<div class="desc"><p>Default backend for NumPy arrays and SciPy objects.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="phi.math.abs"><code class="name flex">
<span>def <span class="ident">abs</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def abs(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.abs)</code></pre>
</details>
</dd>
<dt id="phi.math.abs_square"><code class="name flex">
<span>def <span class="ident">abs_square</span></span>(<span>complex)</span>
</code></dt>
<dd>
<div class="desc"><p>get the square magnitude</p>
<h2 id="args">Args</h2>
<p>complex(Tensor): complex input data</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dt>
<dd>real valued magnitude squared</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def abs_square(complex):
    &#34;&#34;&#34;
    get the square magnitude

    Args:
      complex(Tensor): complex input data

    Returns:
      Tensor: real valued magnitude squared

    &#34;&#34;&#34;
    return math.imag(complex) ** 2 + math.real(complex) ** 2</code></pre>
</details>
</dd>
<dt id="phi.math.all"><code class="name flex">
<span>def <span class="ident">all</span></span>(<span>boolean_tensor: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_(boolean_tensor: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(boolean_tensor, dim,
                   native_function=lambda backend, native, dim: backend.all(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.all_available"><code class="name flex">
<span>def <span class="ident">all_available</span></span>(<span>*values: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Tests if the values of all given tensors are known and can be read at this point.</p>
<p>Tensors are typically available when the backend operates in eager mode.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>tensors to check</dd>
<dt><strong><code>*values</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>bool</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_available(*values: Tensor):
    &#34;&#34;&#34;
    Tests if the values of all given tensors are known and can be read at this point.
    
    Tensors are typically available when the backend operates in eager mode.

    Args:
      values: tensors to check
      *values: Tensor: 

    Returns:
      bool

    &#34;&#34;&#34;
    for value in values:
        natives = value._natives()
        natives_available = [choose_backend(native).is_available(native) for native in natives]
        if not all(natives_available):
            return False
    return True</code></pre>
</details>
</dd>
<dt id="phi.math.any"><code class="name flex">
<span>def <span class="ident">any</span></span>(<span>boolean_tensor: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def any_(boolean_tensor: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(boolean_tensor, dim,
                   native_function=lambda backend, native, dim: backend.any(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.assert_close"><code class="name flex">
<span>def <span class="ident">assert_close</span></span>(<span>*tensors, rel_tolerance=1e-05, abs_tolerance=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks that all tensors have equal values within the specified tolerance.
Raises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.</p>
<p>Does not check that the shapes exactly match.
Tensors with different shapes are reshaped before comparing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensors</code></strong></dt>
<dd>tensor or tensor-like (constant) each</dd>
<dt><strong><code>rel_tolerance</code></strong></dt>
<dd>relative tolerance (Default value = 1e-5)</dd>
<dt><strong><code>abs_tolerance</code></strong></dt>
<dd>absolute tolerance (Default value = 0)</dd>
<dt><strong><code>*tensors</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assert_close(*tensors, rel_tolerance=1e-5, abs_tolerance=0):
    &#34;&#34;&#34;
    Checks that all tensors have equal values within the specified tolerance.
    Raises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.
    
    Does not check that the shapes exactly match.
    Tensors with different shapes are reshaped before comparing.

    Args:
      tensors: tensor or tensor-like (constant) each
      rel_tolerance: relative tolerance (Default value = 1e-5)
      abs_tolerance: absolute tolerance (Default value = 0)
      *tensors: 

    Returns:

    &#34;&#34;&#34;
    any_tensor = next(filter(lambda t: isinstance(t, Tensor), tensors))
    if any_tensor is None:
        tensors = [tensor(t) for t in tensors]
    else:  # use Tensor to infer dimensions
        tensors = [any_tensor._tensor(t) for t in tensors]
    tensors = [t.tensor if isinstance(t, CollapsedTensor) else t for t in tensors]
    for other in tensors[1:]:
        _assert_close(tensors[0], other, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance)</code></pre>
</details>
</dd>
<dt id="phi.math.batch_shape"><code class="name flex">
<span>def <span class="ident">batch_shape</span></span>(<span>sizes: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a> or dict or tuple or list, names: tuple or list = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape with the following properties:</p>
<ul>
<li>All dimensions are of type 'batch'</li>
<li>The shape's <code>names</code> match <code>names</code>, if provided</li>
</ul>
<p>Depending on the type of <code>sizes</code>, returns</p>
<ul>
<li>Shape -&gt; (reordered) spatial sub-shape</li>
<li>dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes</li>
<li>tuple/list of sizes -&gt; matches names to sizes and keeps order</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>list of integers or dict or Shape</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Order of dimensions. Optional if isinstance(sizes, (dict, Shape))</dd>
<dt><strong><code>sizes</code></strong></dt>
<dd>Shape or dict or tuple or list: </dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_shape(sizes: Shape or dict or tuple or list, names: tuple or list = None):
    &#34;&#34;&#34;
    Creates a Shape with the following properties:
    
    * All dimensions are of type &#39;batch&#39;
    * The shape&#39;s `names` match `names`, if provided
    
    Depending on the type of `sizes`, returns
    
    * Shape -&gt; (reordered) spatial sub-shape
    * dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes
    * tuple/list of sizes -&gt; matches names to sizes and keeps order

    Args:
      sizes: list of integers or dict or Shape
      names: Order of dimensions. Optional if isinstance(sizes, (dict, Shape))
      sizes: Shape or dict or tuple or list: 
      names: tuple or list:  (Default value = None)

    Returns:
      Shape containing only spatial dimensions

    &#34;&#34;&#34;
    return _pure_shape(sizes, names, BATCH_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.batch_stack"><code class="name flex">
<span>def <span class="ident">batch_stack</span></span>(<span>values, dim: str = 'batch')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_stack(values, dim: str = &#39;batch&#39;):
    return _stack(values, dim, BATCH_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.boolean_mask"><code class="name flex">
<span>def <span class="ident">boolean_mask</span></span>(<span>x: phi.math._tensors.Tensor, mask)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def boolean_mask(x: Tensor, mask):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.cast"><code class="name flex">
<span>def <span class="ident">cast</span></span>(<span>x: phi.math._tensors.Tensor, dtype: phi.math.backend._dtype.DType) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cast(x: Tensor, dtype: DType) -&gt; Tensor:
    return x._op1(lambda native: choose_backend(native).cast(native, dtype=dtype))</code></pre>
</details>
</dd>
<dt id="phi.math.ceil"><code class="name flex">
<span>def <span class="ident">ceil</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ceil(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.ceil)</code></pre>
</details>
</dd>
<dt id="phi.math.channel_shape"><code class="name flex">
<span>def <span class="ident">channel_shape</span></span>(<span>sizes: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a> or dict or list or tuple, names: tuple or list = None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape with the following properties:</p>
<ul>
<li>All dimensions are of type 'channel'</li>
<li>The shape's <code>names</code> match <code>names</code>, if provided</li>
</ul>
<p>Depending on the type of <code>sizes</code>, returns</p>
<ul>
<li>Shape -&gt; (reordered) spatial sub-shape</li>
<li>dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes</li>
<li>tuple/list of sizes -&gt; matches names to sizes and keeps order</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>list of integers or dict or Shape</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Order of dimensions. Optional if isinstance(sizes, (dict, Shape))</dd>
<dt><strong><code>sizes</code></strong></dt>
<dd>Shape or dict or list or tuple: </dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def channel_shape(sizes: Shape or dict or list or tuple, names: tuple or list = None) -&gt; Shape:
    &#34;&#34;&#34;
    Creates a Shape with the following properties:
    
    * All dimensions are of type &#39;channel&#39;
    * The shape&#39;s `names` match `names`, if provided
    
    Depending on the type of `sizes`, returns
    
    * Shape -&gt; (reordered) spatial sub-shape
    * dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes
    * tuple/list of sizes -&gt; matches names to sizes and keeps order

    Args:
      sizes: list of integers or dict or Shape
      names: Order of dimensions. Optional if isinstance(sizes, (dict, Shape))
      sizes: Shape or dict or list or tuple: 
      names: tuple or list:  (Default value = None)

    Returns:
      Shape containing only spatial dimensions

    &#34;&#34;&#34;
    return _pure_shape(sizes, names, SPATIAL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.channel_stack"><code class="name flex">
<span>def <span class="ident">channel_stack</span></span>(<span>values, dim: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def channel_stack(values, dim: str):
    return _stack(values, dim, CHANNEL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.choose_backend"><code class="name flex">
<span>def <span class="ident">choose_backend</span></span>(<span>*values, prefer_default=False, raise_error=True) ‑> phi.math.backend._backend.Backend</span>
</code></dt>
<dd>
<div class="desc"><p>Selects a suitable backend to handle the given values.</p>
<p>This function is used by most math functions operating on <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> objects to delegate the actual computations.</p>
<h2 id="args">Args</h2>
<dl>
<dt>*values:</dt>
<dt><strong><code>prefer_default</code></strong></dt>
<dd>if True, selects the default backend assuming it can handle handle the values, see <code>default_backend()</code>.</dd>
<dt><strong><code>raise_error</code></strong></dt>
<dd>Determines the behavior of this function if no backend can handle the given values.
If True, raises a <code>NoBackendFound</code> error, else returns <code>None</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>the selected <code>Backend</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def choose_backend(*values, prefer_default=False, raise_error=True) -&gt; Backend:
    &#34;&#34;&#34;
    Selects a suitable backend to handle the given values.

    This function is used by most math functions operating on `Tensor` objects to delegate the actual computations.

    Args:
        *values:
        prefer_default: if True, selects the default backend assuming it can handle handle the values, see `default_backend()`.
        raise_error: Determines the behavior of this function if no backend can handle the given values.
            If True, raises a `NoBackendFound` error, else returns `None`.

    Returns:
        the selected `Backend`
    &#34;&#34;&#34;
    # --- Default Backend has priority ---
    if _is_specific(_DEFAULT[-1], values):
        return _DEFAULT[-1]
    if prefer_default and _is_applicable(_DEFAULT[-1], values):
        return _DEFAULT[-1]
    # --- Filter out non-applicable ---
    backends = [backend for backend in BACKENDS if _is_applicable(backend, values)]
    if len(backends) == 0:
        if raise_error:
            raise NoBackendFound(&#39;No backend found for values %s; registered backends are %s&#39; % (values, BACKENDS))
        else:
            return None
    # --- Native tensors? ---
    for backend in backends:
        if _is_specific(backend, values):
            return backend
    else:
        return backends[0]</code></pre>
</details>
</dd>
<dt id="phi.math.clip"><code class="name flex">
<span>def <span class="ident">clip</span></span>(<span>x: phi.math._tensors.Tensor, lower_limit: float, upper_limit: float)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clip(x: Tensor, lower_limit: float or Tensor, upper_limit: float or Tensor):
    if isinstance(lower_limit, Number) and isinstance(upper_limit, Number):

        def clip_(x):
            return x._op1(lambda native: choose_backend(native).clip(native, lower_limit, upper_limit))

        return broadcast_op(clip_, [x])
    else:
        return maximum(lower_limit, minimum(x, upper_limit))</code></pre>
</details>
</dd>
<dt id="phi.math.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>*tensors, rel_tolerance=1e-05, abs_tolerance=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether all tensors have equal values within the specified tolerance.</p>
<p>Does not check that the shapes exactly match.
Tensors with different shapes are reshaped before comparing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensors</code></strong></dt>
<dd>tensor or tensor-like (constant) each</dd>
<dt><strong><code>rel_tolerance</code></strong></dt>
<dd>relative tolerance (Default value = 1e-5)</dd>
<dt><strong><code>abs_tolerance</code></strong></dt>
<dd>absolute tolerance (Default value = 0)</dd>
<dt><strong><code>*tensors</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(*tensors, rel_tolerance=1e-5, abs_tolerance=0):
    &#34;&#34;&#34;
    Checks whether all tensors have equal values within the specified tolerance.
    
    Does not check that the shapes exactly match.
    Tensors with different shapes are reshaped before comparing.

    Args:
      tensors: tensor or tensor-like (constant) each
      rel_tolerance: relative tolerance (Default value = 1e-5)
      abs_tolerance: absolute tolerance (Default value = 0)
      *tensors: 

    Returns:

    &#34;&#34;&#34;
    tensors = [tensor(t) for t in tensors]
    for other in tensors[1:]:
        if not _close(tensors[0], other, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance):
            return False
    return True</code></pre>
</details>
</dd>
<dt id="phi.math.closest_grid_values"><code class="name flex">
<span>def <span class="ident">closest_grid_values</span></span>(<span>grid: phi.math._tensors.Tensor, coordinates: phi.math._tensors.Tensor, extrap: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a>, stack_dim_prefix='closest_')</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the neighboring grid points in all spatial directions and returns their values.
The result will have 2^d values for each vector in coordiantes in d dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>grid data. The grid is spanned by the spatial dimensions of the tensor</dd>
<dt><strong><code>coordinates</code></strong></dt>
<dd>tensor with 1 channel dimension holding vectors pointing to locations in grid index space</dd>
<dt><strong><code>extrap</code></strong></dt>
<dd>grid extrapolation</dd>
<dt><strong><code>stack_dim_prefix</code></strong></dt>
<dd>For each spatial dimension <code>dim</code>, stacks lower and upper closest values along dimension <code>stack_dim_prefix+dim</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor of shape (batch, coord_spatial, grid_spatial=(2, 2,&hellip;), grid_channel)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def closest_grid_values(grid: Tensor,
                        coordinates: Tensor,
                        extrap: &#39;extrapolation.Extrapolation&#39;,
                        stack_dim_prefix=&#39;closest_&#39;):
    &#34;&#34;&#34;
    Finds the neighboring grid points in all spatial directions and returns their values.
    The result will have 2^d values for each vector in coordiantes in d dimensions.

    Args:
      grid: grid data. The grid is spanned by the spatial dimensions of the tensor
      coordinates: tensor with 1 channel dimension holding vectors pointing to locations in grid index space
      extrap: grid extrapolation
      stack_dim_prefix: For each spatial dimension `dim`, stacks lower and upper closest values along dimension `stack_dim_prefix+dim`.

    Returns:
      Tensor of shape (batch, coord_spatial, grid_spatial=(2, 2,...), grid_channel)

    &#34;&#34;&#34;
    return broadcast_op(functools.partial(_closest_grid_values, extrap=extrap, stack_dim_prefix=stack_dim_prefix), [grid, coordinates])</code></pre>
</details>
</dd>
<dt id="phi.math.concat"><code class="name flex">
<span>def <span class="ident">concat</span></span>(<span>values: tuple, dim: str) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Concatenates a sequence of tensors along one dimension.
The shapes of all values must be equal, except for the size of the concat dimension.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors to concatenate</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>concat dimension, must be present in all values</dd>
<dt><strong><code>values</code></strong></dt>
<dd>tuple or list: </dd>
<dt><strong><code>dim</code></strong></dt>
<dd>str: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>concatenated tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def concat(values: tuple or list, dim: str) -&gt; Tensor:
    &#34;&#34;&#34;
    Concatenates a sequence of tensors along one dimension.
    The shapes of all values must be equal, except for the size of the concat dimension.

    Args:
      values: Tensors to concatenate
      dim: concat dimension, must be present in all values
      values: tuple or list: 
      dim: str: 

    Returns:
      concatenated tensor

    &#34;&#34;&#34;
    broadcast_shape = values[0].shape
    natives = [v.native(order=broadcast_shape.names) for v in values]
    backend = choose_backend(*natives)
    concatenated = backend.concat(natives, broadcast_shape.index(dim))
    return NativeTensor(concatenated, broadcast_shape.with_sizes(backend.staticshape(concatenated)))</code></pre>
</details>
</dd>
<dt id="phi.math.conv"><code class="name flex">
<span>def <span class="ident">conv</span></span>(<span>value: phi.math._tensors.Tensor, kernel: phi.math._tensors.Tensor, padding='same')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv(value: Tensor, kernel: Tensor, padding=&#39;same&#39;):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.cos"><code class="name flex">
<span>def <span class="ident">cos</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cos(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.cos)</code></pre>
</details>
</dd>
<dt id="phi.math.cross_product"><code class="name flex">
<span>def <span class="ident">cross_product</span></span>(<span>vec1: phi.math._tensors.Tensor, vec2: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cross_product(vec1: Tensor, vec2: Tensor):
    vec1, vec2 = math.tensors(vec1, vec2)
    spatial_rank = vec1.vector.size if &#39;vector&#39; in vec1.shape else vec2.vector.size
    if spatial_rank == 2:  # Curl in 2D
        dist_0, dist_1 = vec2.vector.unstack()
        if GLOBAL_AXIS_ORDER.is_x_first:
            velocity = vec1 * math.channel_stack([-dist_1, dist_0], &#39;vector&#39;)
        else:
            velocity = vec1 * math.channel_stack([dist_1, -dist_0], &#39;vector&#39;)
        return velocity
    elif spatial_rank == 3:  # Curl in 3D
        raise NotImplementedError(f&#39;spatial_rank={spatial_rank} not yet implemented&#39;)
    else:
        raise AssertionError(f&#39;dims = {spatial_rank}. Vector product not available in &gt; 3 dimensions&#39;)</code></pre>
</details>
</dd>
<dt id="phi.math.divide_no_nan"><code class="name flex">
<span>def <span class="ident">divide_no_nan</span></span>(<span>x, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def divide_no_nan(x, y):
    return custom_op2(x, y, divide_no_nan, lambda x_, y_: choose_backend(x_, y_).divide_no_nan(x_, y_), lambda y_, x_: divide_no_nan(x_, y_), lambda y_, x_: choose_backend(x_, y_).divide_no_nan(x_, y_))</code></pre>
</details>
</dd>
<dt id="phi.math.dot"><code class="name flex">
<span>def <span class="ident">dot</span></span>(<span>a, b, axes) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dot(a, b, axes) -&gt; Tensor:
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.downsample2x"><code class="name flex">
<span>def <span class="ident">downsample2x</span></span>(<span>grid: phi.math._tensors.Tensor, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Resamples a regular grid to half the number of spatial sample points per dimension.
The grid values at the new points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>full size grid</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>grid extrapolation. Used to insert an additional value for odd spatial dims</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>dims along which down-sampling is applied. If None, down-sample along all spatial dims.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>half-size grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def downsample2x(grid: Tensor,
                 padding: Extrapolation = extrapolation.BOUNDARY,
                 dims: tuple or None = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Resamples a regular grid to half the number of spatial sample points per dimension.
    The grid values at the new points are determined via linear interpolation.

    Args:
      grid: full size grid
      padding: grid extrapolation. Used to insert an additional value for odd spatial dims
      dims: dims along which down-sampling is applied. If None, down-sample along all spatial dims.
      grid: Tensor: 
      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)

    Returns:
      half-size grid

    &#34;&#34;&#34;
    dims = grid.shape.spatial.only(dims).names
    odd_dimensions = [dim for dim in dims if grid.shape.get_size(dim) % 2 != 0]
    grid = math.pad(grid, {dim: (0, 1) for dim in odd_dimensions}, padding)
    for dim in dims:
        grid = (grid[{dim: slice(1, None, 2)}] + grid[{dim: slice(0, None, 2)}]) / 2
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.dtype"><code class="name flex">
<span>def <span class="ident">dtype</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dtype(x):
    if isinstance(x, Tensor):
        return x.dtype
    else:
        return choose_backend(x).dtype(x)</code></pre>
</details>
</dd>
<dt id="phi.math.einsum"><code class="name flex">
<span>def <span class="ident">einsum</span></span>(<span>equation, *tensors) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def einsum(equation, *tensors) -&gt; Tensor:
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.exp"><code class="name flex">
<span>def <span class="ident">exp</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exp(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.exp)</code></pre>
</details>
</dd>
<dt id="phi.math.expand_channel"><code class="name flex">
<span>def <span class="ident">expand_channel</span></span>(<span>value: phi.math._tensors.Tensor, dim_name: str, dim_size: int = 1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_channel(value: Tensor, dim_name: str, dim_size: int = 1):
    return _expand(value, dim_name, dim_size, CHANNEL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.fft"><code class="name flex">
<span>def <span class="ident">fft</span></span>(<span>x: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a fast Fourier transform (FFT) on all spatial dimensions of x.</p>
<p>The inverse operation is :func:<code><a title="phi.math.ifft" href="#phi.math.ifft">ifft()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>tensor of type float or complex</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>FFT(x) of type complex</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fft(x: Tensor):
    &#34;&#34;&#34;
    Performs a fast Fourier transform (FFT) on all spatial dimensions of x.
    
    The inverse operation is :func:`ifft`.

    Args:
      x: tensor of type float or complex
      x: Tensor: 

    Returns:
      FFT(x) of type complex

    &#34;&#34;&#34;
    native, assemble = _invertible_standard_form(x)
    result = choose_backend(native).fft(native)
    return assemble(result)</code></pre>
</details>
</dd>
<dt id="phi.math.fftfreq"><code class="name flex">
<span>def <span class="ident">fftfreq</span></span>(<span>resolution, dtype=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the discrete Fourier transform sample frequencies.
These are the frequencies corresponding to the components of the result of <code>math.fft</code> on a tensor of shape <code>resolution</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>resolution</code></strong></dt>
<dd>grid resolution measured in cells</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>data type of the returned tensor (Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor holding the frequencies of the corresponding values computed by math.fft</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fftfreq(resolution, dtype=None):
    &#34;&#34;&#34;
    Returns the discrete Fourier transform sample frequencies.
    These are the frequencies corresponding to the components of the result of `math.fft` on a tensor of shape `resolution`.

    Args:
      resolution: grid resolution measured in cells
      dtype: data type of the returned tensor (Default value = None)

    Returns:
      tensor holding the frequencies of the corresponding values computed by math.fft

    &#34;&#34;&#34;
    resolution = spatial_shape(resolution)
    k = meshgrid(**{dim: np.fft.fftfreq(int(n)) for dim, n in resolution.named_sizes})
    return to_float(k) if dtype is None else cast(k, dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.floor"><code class="name flex">
<span>def <span class="ident">floor</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def floor(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.floor)</code></pre>
</details>
</dd>
<dt id="phi.math.fourier_laplace"><code class="name flex">
<span>def <span class="ident">fourier_laplace</span></span>(<span>grid: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor, times: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies the spatial laplace operator to the given tensor with periodic boundary conditions.</p>
<p><em>Note:</em> The results of <code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace()</a></code> and <code><a title="phi.math.laplace" href="#phi.math.laplace">laplace()</a></code> are close but not identical.</p>
<p>This implementation computes the laplace operator in Fourier space.
The result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>tensor, assumed to have periodic boundary conditions</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>distance between grid points, tensor-like, scalar or vector</dd>
<dt><strong><code>times</code></strong></dt>
<dd>number of times the laplace operator is applied. The computational cost is independent of this parameter.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or Shape or float or list or tuple: </dd>
<dt><strong><code>times</code></strong></dt>
<dd>int:
(Default value = 1)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of same shape as <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fourier_laplace(grid: Tensor,
                    dx: Tensor or Shape or float or list or tuple,
                    times: int = 1):
    &#34;&#34;&#34;
    Applies the spatial laplace operator to the given tensor with periodic boundary conditions.
    
    *Note:* The results of `fourier_laplace` and `laplace` are close but not identical.
    
    This implementation computes the laplace operator in Fourier space.
    The result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.

    Args:
      grid: tensor, assumed to have periodic boundary conditions
      dx: distance between grid points, tensor-like, scalar or vector
      times: number of times the laplace operator is applied. The computational cost is independent of this parameter.
      grid: Tensor: 
      dx: Tensor or Shape or float or list or tuple: 
      times: int:  (Default value = 1)

    Returns:
      tensor of same shape as `tensor`

    &#34;&#34;&#34;
    frequencies = math.fft(math.to_complex(grid))
    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, &#39;vector&#39;)
    fft_laplace = -(2 * np.pi)**2 * k_squared
    result = math.real(math.ifft(frequencies * fft_laplace ** times))
    return math.cast(result / tensor(dx) ** 2, grid.dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.fourier_poisson"><code class="name flex">
<span>def <span class="ident">fourier_poisson</span></span>(<span>grid: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor, times: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse operation to <code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or Shape or float or list or tuple: </dd>
<dt><strong><code>times</code></strong></dt>
<dd>int:
(Default value = 1)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fourier_poisson(grid: Tensor,
                    dx: Tensor or Shape or float or list or tuple,
                    times: int = 1):
    &#34;&#34;&#34;
    Inverse operation to `fourier_laplace`.

    Args:
      grid: Tensor: 
      dx: Tensor or Shape or float or list or tuple: 
      times: int:  (Default value = 1)

    Returns:

    &#34;&#34;&#34;
    frequencies = math.fft(math.to_complex(grid))
    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, &#39;vector&#39;)
    fft_laplace = -(2 * np.pi)**2 * k_squared
    # fft_laplace.tensor[(0,) * math.ndims(k_squared)] = math.inf  # assume NumPy array to edit
    result = math.real(math.ifft(math.divide_no_nan(frequencies, math.to_complex(fft_laplace ** times))))
    return math.cast(result * tensor(dx) ** 2, grid.dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.frequency_loss"><code class="name flex">
<span>def <span class="ident">frequency_loss</span></span>(<span>tensor, frequency_falloff=100, reduce_batches=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Instead of minimizing each entry of the tensor, minimize the frequencies of the tensor, emphasizing lower frequencies over higher ones.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>reduce_batches</code></strong></dt>
<dd>whether to reduce the batch dimension of the loss by adding the losses along the first dimension (Default value = True)</dd>
<dt><strong><code>tensor</code></strong></dt>
<dd>typically actual - target</dd>
<dt><strong><code>frequency_falloff</code></strong></dt>
<dd>large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally. (Default value = 100)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>scalar loss value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def frequency_loss(tensor, frequency_falloff=100, reduce_batches=True):
    &#34;&#34;&#34;
    Instead of minimizing each entry of the tensor, minimize the frequencies of the tensor, emphasizing lower frequencies over higher ones.

    Args:
      reduce_batches: whether to reduce the batch dimension of the loss by adding the losses along the first dimension (Default value = True)
      tensor: typically actual - target
      frequency_falloff: large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally. (Default value = 100)

    Returns:
      scalar loss value

    &#34;&#34;&#34;
    if struct.isstruct(tensor):
        all_tensors = struct.flatten(tensor)
        return sum(frequency_loss(tensor, frequency_falloff, reduce_batches) for tensor in all_tensors)
    diff_fft = abs_square(math.fft(tensor))
    k_squared = math.sum_(math.fftfreq(tensor.shape[1:-1]) ** 2, &#39;vector&#39;)
    weights = math.exp(-0.5 * k_squared * frequency_falloff ** 2)
    return l1_loss(diff_fft * weights, reduce_batches=reduce_batches)</code></pre>
</details>
</dd>
<dt id="phi.math.get_precision"><code class="name flex">
<span>def <span class="ident">get_precision</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the current target floating point precision in bits.
The precision can be set globally using <code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision()</a></code> or locally using <code>with precision(p):</code>.</p>
<p>Any Backend method may convert floating point values to this precision, even if the input had a different precision.</p>
<h2 id="returns">Returns</h2>
<p>16 for half, 32 for single, 64 for double</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_precision() -&gt; int:
    &#34;&#34;&#34;
    Gets the current target floating point precision in bits.
    The precision can be set globally using `set_global_precision()` or locally using `with precision(p):`.

    Any Backend method may convert floating point values to this precision, even if the input had a different precision.

    Returns:
        16 for half, 32 for single, 64 for double
    &#34;&#34;&#34;
    return _PRECISION[-1]</code></pre>
</details>
</dd>
<dt id="phi.math.gradient"><code class="name flex">
<span>def <span class="ident">gradient</span></span>(<span>grid: phi.math._tensors.Tensor, dx: float = 1, difference: str = 'central', padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None, stack_dim: str = 'gradient')</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the gradient of a scalar channel from finite differences.
The gradient vectors are in reverse order, lowest dimension first.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>grid values</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>optional) sequence of dimension names</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>physical distance between grid points (default 1)</dd>
<dt><strong><code>difference</code></strong></dt>
<dd>type of difference, one of ('forward', 'backward', 'central') (default 'forward')</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>tensor padding mode</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>name of the new vector dimension listing the gradient w.r.t. the various axes</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>float or int:
(Default value = 1)</dd>
<dt><strong><code>difference</code></strong></dt>
<dd>str:
(Default value = 'central')</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation or None:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>str:
(Default value = 'gradient')</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of shape (batch_size, spatial_dimensions&hellip;, spatial rank)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gradient(grid: Tensor,
             dx: float or int = 1,
             difference: str = &#39;central&#39;,
             padding: Extrapolation or None = extrapolation.BOUNDARY,
             dims: tuple or None = None,
             stack_dim: str = &#39;gradient&#39;):
    &#34;&#34;&#34;
    Calculates the gradient of a scalar channel from finite differences.
    The gradient vectors are in reverse order, lowest dimension first.

    Args:
      grid: grid values
      dims: optional) sequence of dimension names
      dx: physical distance between grid points (default 1)
      difference: type of difference, one of (&#39;forward&#39;, &#39;backward&#39;, &#39;central&#39;) (default &#39;forward&#39;)
      padding: tensor padding mode
      stack_dim: name of the new vector dimension listing the gradient w.r.t. the various axes
      grid: Tensor: 
      dx: float or int:  (Default value = 1)
      difference: str:  (Default value = &#39;central&#39;)
      padding: Extrapolation or None:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)
      stack_dim: str:  (Default value = &#39;gradient&#39;)

    Returns:
      tensor of shape (batch_size, spatial_dimensions..., spatial rank)

    &#34;&#34;&#34;
    grid = tensor(grid)
    if difference.lower() == &#39;central&#39;:
        left, right = shift(grid, (-1, 1), dims, padding, stack_dim=stack_dim)
        return (right - left) / (dx * 2)
    elif difference.lower() == &#39;forward&#39;:
        left, right = shift(grid, (0, 1), dims, padding, stack_dim=stack_dim)
        return (right - left) / dx
    elif difference.lower() == &#39;backward&#39;:
        left, right = shift(grid, (-1, 0), dims, padding, stack_dim=stack_dim)
        return (right - left) / dx
    else:
        raise ValueError(&#39;Invalid difference type: {}. Can be CENTRAL or FORWARD&#39;.format(difference))</code></pre>
</details>
</dd>
<dt id="phi.math.grid_sample"><code class="name flex">
<span>def <span class="ident">grid_sample</span></span>(<span>grid: phi.math._tensors.Tensor, coordinates: phi.math._tensors.Tensor, extrap: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def grid_sample(grid: Tensor, coordinates: Tensor, extrap: &#39;extrapolation.Extrapolation&#39;):
    result = broadcast_op(functools.partial(_grid_sample, extrap=extrap), [grid, coordinates])
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.ifft"><code class="name flex">
<span>def <span class="ident">ifft</span></span>(<span>k: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ifft(k: Tensor):
    native, assemble = _invertible_standard_form(k)
    result = choose_backend(native).ifft(native)
    return assemble(result)</code></pre>
</details>
</dd>
<dt id="phi.math.imag"><code class="name flex">
<span>def <span class="ident">imag</span></span>(<span>complex: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def imag(complex: Tensor) -&gt; Tensor:
    return _backend_op1(complex, Backend.imag)</code></pre>
</details>
</dd>
<dt id="phi.math.isfinite"><code class="name flex">
<span>def <span class="ident">isfinite</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def isfinite(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.isfinite)</code></pre>
</details>
</dd>
<dt id="phi.math.join_dimensions"><code class="name flex">
<span>def <span class="ident">join_dimensions</span></span>(<span>value: phi.math._tensors.Tensor, dims: phi.math._shape.Shape, joined_dim_name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Stacks multiple dimensions into a single dimension.</p>
<h2 id="args">Args</h2>
<p>value:
dims:
joined_dim_name:
Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def join_dimensions(value: Tensor, dims: Shape or tuple or list, joined_dim_name: str):
    &#34;&#34;&#34;
    Stacks multiple dimensions into a single dimension.

    Args:
        value:
        dims:
        joined_dim_name:

    Returns:

    &#34;&#34;&#34;
    if len(dims) == 0:
        return CollapsedTensor(value, value.shape.expand(1, joined_dim_name, _infer_dim_type_from_name(joined_dim_name)))
    order = value.shape.order_group(dims)
    native = value.native(order)
    types = value.shape.get_type(dims)
    dim_type = types[0] if len(set(types)) == 1 else BATCH_DIM
    first_dim_index = min(value.shape.indices(dims))
    new_shape = value.shape.without(dims).expand(value.shape.only(dims).volume, joined_dim_name, dim_type, pos=first_dim_index)
    native = choose_backend(native).reshape(native, new_shape.sizes)
    return NativeTensor(native, new_shape)</code></pre>
</details>
</dd>
<dt id="phi.math.l1_loss"><code class="name flex">
<span>def <span class="ident">l1_loss</span></span>(<span>tensor: phi.math._tensors.Tensor, batch_norm=True, reduce_batches=True)</span>
</code></dt>
<dd>
<div class="desc"><p>get L1 loss</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>(Default value = True)</dd>
<dt><strong><code>reduce_batches</code></strong></dt>
<dd>(Default value = True)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l1_loss(tensor: Tensor, batch_norm=True, reduce_batches=True):
    &#34;&#34;&#34;
    get L1 loss

    Args:
      tensor: Tensor: 
      batch_norm:  (Default value = True)
      reduce_batches:  (Default value = True)

    Returns:

    &#34;&#34;&#34;
    if struct.isstruct(tensor):
        all_tensors = struct.flatten(tensor)
        return sum(l1_loss(tensor, batch_norm, reduce_batches) for tensor in all_tensors)
    if reduce_batches:
        total_loss = math.sum_(math.abs(tensor))
    else:
        total_loss = math.sum_(math.abs(tensor), dim=list(range(1, len(tensor.shape))))
    if batch_norm and reduce_batches:
        batch_size = tensor.shape.sizes[0]
        return math.divide_no_nan(total_loss, math.to_float(batch_size))
    else:
        return total_loss</code></pre>
</details>
</dd>
<dt id="phi.math.l2_loss"><code class="name flex">
<span>def <span class="ident">l2_loss</span></span>(<span>tensor: phi.math._tensors.Tensor, batch_norm=True)</span>
</code></dt>
<dd>
<div class="desc"><p>get L2 loss</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>(Default value = True)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l2_loss(tensor: Tensor, batch_norm=True):
    &#34;&#34;&#34;
    get L2 loss

    Args:
      tensor: Tensor: 
      batch_norm:  (Default value = True)

    Returns:

    &#34;&#34;&#34;
    return l_n_loss(tensor, 2, batch_norm=batch_norm)</code></pre>
</details>
</dd>
<dt id="phi.math.l_n_loss"><code class="name flex">
<span>def <span class="ident">l_n_loss</span></span>(<span>tensor: phi.math._tensors.Tensor, n: int, batch_norm=True)</span>
</code></dt>
<dd>
<div class="desc"><p>get Ln loss</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>n</code></strong></dt>
<dd>int: </dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>(Default value = True)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l_n_loss(tensor: Tensor, n: int, batch_norm=True):
    &#34;&#34;&#34;
    get Ln loss

    Args:
      tensor: Tensor: 
      n: int: 
      batch_norm:  (Default value = True)

    Returns:

    &#34;&#34;&#34;
    if struct.isstruct(tensor):
        all_tensors = struct.flatten(tensor)
        return sum(l_n_loss(tensor, n, batch_norm) for tensor in all_tensors)
    total_loss = math.sum_(tensor ** n) / n
    if batch_norm:
        batch_size = tensor.shape.sizes[0]
        return math.divide_no_nan(total_loss, math.to_float(batch_size))
    else:
        return total_loss</code></pre>
</details>
</dd>
<dt id="phi.math.laplace"><code class="name flex">
<span>def <span class="ident">laplace</span></span>(<span>x: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor = 1, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Spatial Laplace operator as defined for scalar fields.
If a vector field is passed, the laplace is computed component-wise.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>n-dimensional field of shape (batch, spacial dimensions&hellip;, components)</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>scalar or 1d tensor</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>extrapolation</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>The second derivative along these dimensions is summed over</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or float:
(Default value = 1)</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of same shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def laplace(x: Tensor,
            dx: Tensor or float = 1,
            padding: Extrapolation = extrapolation.BOUNDARY,
            dims: tuple or None = None):
    &#34;&#34;&#34;
    Spatial Laplace operator as defined for scalar fields.
    If a vector field is passed, the laplace is computed component-wise.

    Args:
      x: n-dimensional field of shape (batch, spacial dimensions..., components)
      dx: scalar or 1d tensor
      padding: extrapolation
      dims: The second derivative along these dimensions is summed over
      x: Tensor: 
      dx: Tensor or float:  (Default value = 1)
      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)

    Returns:
      tensor of same shape

    &#34;&#34;&#34;
    if not isinstance(dx, (int, float)):
        dx = tensor(dx, names=&#39;_laplace&#39;)
    if isinstance(x, Extrapolation):
        return x.gradient()
    left, center, right = shift(tensor(x), (-1, 0, 1), dims, padding, stack_dim=&#39;_laplace&#39;)
    result = (left + right - 2 * center) / dx
    result = math.sum_(result, &#39;_laplace&#39;)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.linspace"><code class="name flex">
<span>def <span class="ident">linspace</span></span>(<span>start, stop, number: int, dim='linspace')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def linspace(start, stop, number: int, dim=&#39;linspace&#39;):
    native = choose_backend(start, stop, number, prefer_default=True).linspace(start, stop, number)
    return NativeTensor(native, shape_(**{dim: number}))</code></pre>
</details>
</dd>
<dt id="phi.math.matmul"><code class="name flex">
<span>def <span class="ident">matmul</span></span>(<span>A, b) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def matmul(A, b) -&gt; Tensor:
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.max"><code class="name flex">
<span>def <span class="ident">max</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.max(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.maximum"><code class="name flex">
<span>def <span class="ident">maximum</span></span>(<span>x: phi.math._tensors.Tensor, y: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def maximum(x: Tensor or float, y: Tensor or float):
    return custom_op2(x, y, maximum, lambda x_, y_: choose_backend(x_, y_).maximum(x_, y_))</code></pre>
</details>
</dd>
<dt id="phi.math.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.mean(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>**dimensions)</span>
</code></dt>
<dd>
<div class="desc"><p>generate a TensorStack meshgrid from keyword dimensions</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>**dimensions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(**dimensions):
    &#34;&#34;&#34;
    generate a TensorStack meshgrid from keyword dimensions

    Args:
      **dimensions: 

    Returns:

    &#34;&#34;&#34;
    assert &#39;vector&#39; not in dimensions
    dim_values = {}
    for dim, spec in dimensions.items():
        if isinstance(spec, int):
            dim_values[dim] = tuple(range(spec))
        elif isinstance(spec, Tensor):
            dim_values[dim] = spec.native()
        else:
            dim_values[dim] = spec
    backend = choose_backend(*dim_values.values(), prefer_default=True)
    indices_list = backend.meshgrid(*dim_values.values())
    single_shape = Shape([len(val) for val in dim_values.values()], dim_values.keys(), [SPATIAL_DIM] * len(dim_values))
    channels = [NativeTensor(t, single_shape) for t in indices_list]
    return TensorStack(channels, &#39;vector&#39;, CHANNEL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.min"><code class="name flex">
<span>def <span class="ident">min</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def min_(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.min(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.minimum"><code class="name flex">
<span>def <span class="ident">minimum</span></span>(<span>x: phi.math._tensors.Tensor, y: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def minimum(x: Tensor or float, y: Tensor or float):
    return custom_op2(x, y, minimum, lambda x_, y_: choose_backend(x_, y_).minimum(x_, y_))</code></pre>
</details>
</dd>
<dt id="phi.math.nonzero"><code class="name flex">
<span>def <span class="ident">nonzero</span></span>(<span>value: phi.math._tensors.Tensor, list_dim='nonzero', index_dim='vector')</span>
</code></dt>
<dd>
<div class="desc"><p>Get spatial indices of non-zero / True values.</p>
<p>Batch dimensions are preserved by this operation.
If channel dimensions are present, this method returns the indices where any entry is nonzero.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>spatial tensor to find non-zero / True values in.</dd>
<dt><strong><code>list_dim</code></strong></dt>
<dd>name of dimension listing non-zero values (Default value = 'nonzero')</dd>
<dt><strong><code>index_dim</code></strong></dt>
<dd>name of index dimension (Default value = 'vector')</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of shape (batch dims&hellip;, list_dim=#non-zero, index_dim=value.shape.spatial_rank)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nonzero(value: Tensor, list_dim=&#39;nonzero&#39;, index_dim=&#39;vector&#39;):
    &#34;&#34;&#34;
    Get spatial indices of non-zero / True values.
    
    Batch dimensions are preserved by this operation.
    If channel dimensions are present, this method returns the indices where any entry is nonzero.

    Args:
      value: spatial tensor to find non-zero / True values in.
      list_dim: name of dimension listing non-zero values (Default value = &#39;nonzero&#39;)
      index_dim: name of index dimension (Default value = &#39;vector&#39;)
      value: Tensor: 

    Returns:
      tensor of shape (batch dims..., list_dim=#non-zero, index_dim=value.shape.spatial_rank)

    &#34;&#34;&#34;
    if value.shape.channel_rank &gt; 0:
        value = sum_(abs(value), value.shape.channel.names)

    def unbatched_nonzero(value):
        native = value.native()
        backend = choose_backend(native)
        indices = backend.nonzero(native)
        indices_shape = Shape(backend.staticshape(indices), (list_dim, index_dim), (BATCH_DIM, CHANNEL_DIM))
        return NativeTensor(indices, indices_shape)

    return broadcast_op(unbatched_nonzero, [value], iter_dims=value.shape.batch.names)</code></pre>
</details>
</dd>
<dt id="phi.math.normalize_to"><code class="name flex">
<span>def <span class="ident">normalize_to</span></span>(<span>target: phi.math._tensors.Tensor, source: phi.math._tensors.Tensor, epsilon=1e-05)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiplies the target so that its total content matches the source.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>target</code></strong></dt>
<dd>a tensor</dd>
<dt><strong><code>source</code></strong></dt>
<dd>a tensor or number</dd>
<dt><strong><code>epsilon</code></strong></dt>
<dd>small number to prevent division by zero or None. (Default value = 1e-5)</dd>
<dt><strong><code>target</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>source</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>normalized tensor of the same shape as target</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_to(target: Tensor, source: Tensor, epsilon=1e-5):
    &#34;&#34;&#34;
    Multiplies the target so that its total content matches the source.

    Args:
      target: a tensor
      source: a tensor or number
      epsilon: small number to prevent division by zero or None. (Default value = 1e-5)
      target: Tensor: 
      source: Tensor: 

    Returns:
      normalized tensor of the same shape as target

    &#34;&#34;&#34;
    target_total = math.sum_(target, dim=target.shape.non_batch.names)
    denominator = math.maximum(target_total, epsilon) if epsilon is not None else target_total
    source_total = math.sum_(source, dim=source.shape.non_batch.names)
    return target * (source_total / denominator)</code></pre>
</details>
</dd>
<dt id="phi.math.ones"><code class="name flex">
<span>def <span class="ident">ones</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"><p>Define a tensor with specified shape with value 1 / True everywhere.</p>
<p>This method may not immediately allocate the memory to store the values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong></dt>
<dd>base tensor shape (Default value = EMPTY_SHAPE)</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>data type (Default value = None)</dd>
<dt><strong><code>dimensions</code></strong></dt>
<dd>additional dimensions, types are determined from names</dd>
<dt><strong><code>**dimensions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of specified shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones(shape=EMPTY_SHAPE, dtype=None, **dimensions):
    &#34;&#34;&#34;
    Define a tensor with specified shape with value 1 / True everywhere.
    
    This method may not immediately allocate the memory to store the values.

    Args:
      shape: base tensor shape (Default value = EMPTY_SHAPE)
      dtype: data type (Default value = None)
      dimensions: additional dimensions, types are determined from names
      **dimensions: 

    Returns:
      tensor of specified shape

    &#34;&#34;&#34;
    return _initialize(lambda shape, dtype: CollapsedTensor(NativeTensor(default_backend().ones((), dtype=dtype), EMPTY_SHAPE), shape), shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.ones_like"><code class="name flex">
<span>def <span class="ident">ones_like</span></span>(<span>tensor: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones_like(tensor: Tensor):
    return zeros(tensor.shape, dtype=tensor.dtype) + 1</code></pre>
</details>
</dd>
<dt id="phi.math.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>value: phi.math._tensors.Tensor, widths: dict, mode: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a>) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Pads a tensor along the specified dimensions, determining the added values using the given extrapolation.</p>
<p>This is equivalent to calling <code>mode.pad(value, widths)</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor to be padded</dd>
<dt><strong><code>widths</code></strong></dt>
<dd>name: str -&gt; (lower: int, upper: int)</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>Extrapolation object</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>widths</code></strong></dt>
<dd>dict: </dd>
<dt><strong><code>mode</code></strong></dt>
<dd>'extrapolation.Extrapolation': </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>padded Tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad(value: Tensor, widths: dict, mode: &#39;extrapolation.Extrapolation&#39;) -&gt; Tensor:
    &#34;&#34;&#34;
    Pads a tensor along the specified dimensions, determining the added values using the given extrapolation.
    
    This is equivalent to calling `mode.pad(value, widths)`.

    Args:
      value: tensor to be padded
      widths: name: str -&gt; (lower: int, upper: int)
      mode: Extrapolation object
      value: Tensor: 
      widths: dict: 
      mode: &#39;extrapolation.Extrapolation&#39;: 

    Returns:
      padded Tensor

    &#34;&#34;&#34;
    return mode.pad(value, widths)</code></pre>
</details>
</dd>
<dt id="phi.math.precision"><code class="name flex">
<span>def <span class="ident">precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision for the local context.</p>
<p>Usage: <code>with precision(p):</code></p>
<p>This overrides the global setting, see <code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>16 for half, 32 for single, 64 for double</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def precision(floating_point_bits: int):
    &#34;&#34;&#34;
    Sets the floating point precision for the local context.

    Usage: `with precision(p):`

    This overrides the global setting, see `set_global_precision()`.

    Args:
        floating_point_bits: 16 for half, 32 for single, 64 for double
    &#34;&#34;&#34;
    _PRECISION.append(floating_point_bits)
    try:
        yield None
    finally:
        _PRECISION.pop(-1)</code></pre>
</details>
</dd>
<dt id="phi.math.print"><code class="name flex">
<span>def <span class="ident">print</span></span>(<span>value: phi.math._tensors.Tensor = None, name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Print a tensor with no more than two spatial dimensions, splitting it along all batch and channel dimensions.</p>
<p>Unlike regular printing, the primary dimension, typically x, is oriented to the right.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>name of the tensor</dd>
<dt><strong><code>value</code></strong></dt>
<dd>tensor-like</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor:
(Default value = None)</dd>
<dt><strong><code>name</code></strong></dt>
<dd>str:
(Default value = None)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_(value: Tensor = None, name: str = None):
    &#34;&#34;&#34;
    Print a tensor with no more than two spatial dimensions, splitting it along all batch and channel dimensions.
    
    Unlike regular printing, the primary dimension, typically x, is oriented to the right.

    Args:
      name: name of the tensor
      value: tensor-like
      value: Tensor:  (Default value = None)
      name: str:  (Default value = None)

    Returns:

    &#34;&#34;&#34;
    if value is None:
        print()
        return
    if name is not None:
        print(&#34; &#34; * 16 + name)
    value = tensor(value)
    dim_order = tuple(sorted(value.shape.spatial.names, reverse=True))
    if value.shape.spatial_rank == 0:
        print(value.numpy())
    elif value.shape.spatial_rank == 1:
        for index_dict in value.shape.non_spatial.meshgrid():
            if value.shape.non_spatial.volume &gt; 1:
                print(f&#34;--- {&#39;, &#39;.join(&#39;%s=%d&#39; % (name, idx) for name, idx in index_dict.items())} ---&#34;)
            text = np.array2string(value[index_dict].numpy(dim_order), precision=2, separator=&#39;, &#39;, max_line_width=np.inf)
            print(&#39; &#39; + re.sub(&#39;[\[\]]&#39;, &#39;&#39;, text))
    elif value.shape.spatial_rank == 2:
        for index_dict in value.shape.non_spatial.meshgrid():
            if value.shape.non_spatial.volume &gt; 1:
                print(f&#34;--- {&#39;, &#39;.join(&#39;%s=%d&#39; % (name, idx) for name, idx in index_dict.items())} ---&#34;)
            text = np.array2string(value[index_dict].numpy(dim_order), precision=2, separator=&#39;, &#39;, max_line_width=np.inf)
            print(&#39; &#39; + re.sub(&#39;[\[\]]&#39;, &#39;&#39;, re.sub(&#39;\],&#39;, &#39;&#39;, text)))
    else:
        raise NotImplementedError(&#39;Can only print tensors with up to 2 spatial dimensions.&#39;)</code></pre>
</details>
</dd>
<dt id="phi.math.prod"><code class="name flex">
<span>def <span class="ident">prod</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prod(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.prod(native, dim),
                   collapsed_function=lambda inner, red_shape: inner ** red_shape.volume)</code></pre>
</details>
</dd>
<dt id="phi.math.random_normal"><code class="name flex">
<span>def <span class="ident">random_normal</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_normal(shape=EMPTY_SHAPE, dtype=None, **dimensions):

    def uniform_random_normal(shape, dtype):
        native = choose_backend(*shape.sizes, prefer_default=True).random_normal(shape.sizes)
        native = native if dtype is None else native.astype(dtype)
        return NativeTensor(native, shape)

    return _initialize(uniform_random_normal, shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.random_uniform"><code class="name flex">
<span>def <span class="ident">random_uniform</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_uniform(shape=EMPTY_SHAPE, dtype=None, **dimensions):

    def uniform_random_uniform(shape, dtype):
        native = choose_backend(*shape.sizes, prefer_default=True).random_uniform(shape.sizes)
        native = native if dtype is None else native.astype(dtype)
        return NativeTensor(native, shape)

    return _initialize(uniform_random_uniform, shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.real"><code class="name flex">
<span>def <span class="ident">real</span></span>(<span>complex: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def real(complex: Tensor) -&gt; Tensor:
    return _backend_op1(complex, Backend.real)</code></pre>
</details>
</dd>
<dt id="phi.math.round"><code class="name flex">
<span>def <span class="ident">round</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def round_(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.round)</code></pre>
</details>
</dd>
<dt id="phi.math.sample_subgrid"><code class="name flex">
<span>def <span class="ident">sample_subgrid</span></span>(<span>grid: phi.math._tensors.Tensor, start: phi.math._tensors.Tensor, size: phi.math._shape.Shape) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Samples a sub-grid from <code>grid</code> with equal distance between sampling points.
The values at the new sample points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>full size grid to be resampled</dd>
<dt><strong><code>start</code></strong></dt>
<dd>origin point of sub-grid within <code>grid</code>, measured in number of cells.</dd>
</dl>
<p>Must have a single dimension called <code>vector</code>.
Example: <code>start=(1, 0.5)</code> would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.
The order of dims must be equal to <code>size</code> and <code>grid.shape.spatial</code>.
size: resolution of the sub-grid. Must not be larger than the resolution of <code>grid</code>.
The order of dims must be equal to <code>start</code> and <code>grid.shape.spatial</code>.
grid: Tensor:
start: Tensor:
size: Shape: </p>
<h2 id="returns">Returns</h2>
<p>sampled sub-grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_subgrid(grid: Tensor, start: Tensor, size: Shape) -&gt; Tensor:
    &#34;&#34;&#34;
    Samples a sub-grid from `grid` with equal distance between sampling points.
    The values at the new sample points are determined via linear interpolation.

    Args:
      grid: full size grid to be resampled
      start: origin point of sub-grid within `grid`, measured in number of cells.
    Must have a single dimension called `vector`.
    Example: `start=(1, 0.5)` would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.
    The order of dims must be equal to `size` and `grid.shape.spatial`.
      size: resolution of the sub-grid. Must not be larger than the resolution of `grid`.
    The order of dims must be equal to `start` and `grid.shape.spatial`.
      grid: Tensor: 
      start: Tensor: 
      size: Shape: 

    Returns:
      sampled sub-grid

    &#34;&#34;&#34;
    assert start.shape.names == (&#39;vector&#39;,)
    assert grid.shape.spatial.names == size.names
    discard = {}
    for dim, d_start, d_size in zip(grid.shape.spatial.names, start, size):
        discard[dim] = slice(int(d_start), int(d_start) + d_size + (1 if d_start != 0 else 0))
    grid = grid[discard]
    upper_weight = start % 1
    lower_weight = 1 - upper_weight
    for i, dim in enumerate(grid.shape.spatial.names):
        if upper_weight[i] not in (0, 1):
            lower, upper = shift(grid, (0, 1), [dim], padding=None, stack_dim=None)
            grid = upper * upper_weight[i] + lower * lower_weight[i]
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.scatter"><code class="name flex">
<span>def <span class="ident">scatter</span></span>(<span>indices: phi.math._tensors.Tensor, values: phi.math._tensors.Tensor, size: phi.math._shape.Shape, scatter_dims, duplicates_handling='undefined', outside_handling='discard')</span>
</code></dt>
<dd>
<div class="desc"><p>Create a dense tensor from sparse values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong></dt>
<dd>n-dimensional indices corresponding to values</dd>
<dt><strong><code>values</code></strong></dt>
<dd>values to scatter at indices</dd>
<dt><strong><code>size</code></strong></dt>
<dd>spatial size of dense tensor</dd>
<dt><strong><code>scatter_dims</code></strong></dt>
<dd>dimensions of values/indices to reduce during scattering</dd>
<dt><strong><code>duplicates_handling</code></strong></dt>
<dd>one of ('undefined', 'add', 'mean', 'any') (Default value = 'undefined')</dd>
<dt><strong><code>outside_handling</code></strong></dt>
<dd>one of ('discard', 'clamp', 'undefined') (Default value = 'discard')</dd>
<dt><strong><code>indices</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>values</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>size</code></strong></dt>
<dd>Shape: </dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scatter(indices: Tensor, values: Tensor, size: Shape, scatter_dims, duplicates_handling=&#39;undefined&#39;, outside_handling=&#39;discard&#39;):
    &#34;&#34;&#34;
    Create a dense tensor from sparse values.

    Args:
      indices: n-dimensional indices corresponding to values
      values: values to scatter at indices
      size: spatial size of dense tensor
      scatter_dims: dimensions of values/indices to reduce during scattering
      duplicates_handling: one of (&#39;undefined&#39;, &#39;add&#39;, &#39;mean&#39;, &#39;any&#39;) (Default value = &#39;undefined&#39;)
      outside_handling: one of (&#39;discard&#39;, &#39;clamp&#39;, &#39;undefined&#39;) (Default value = &#39;discard&#39;)
      indices: Tensor: 
      values: Tensor: 
      size: Shape: 

    Returns:

    &#34;&#34;&#34;
    indices_ = indices.native()
    values_ = values.native(values.shape.combined(indices.shape.non_channel).names)
    backend = choose_backend(indices_, values_)
    result_ = backend.scatter(indices_, values_, tuple(size), duplicates_handling=duplicates_handling, outside_handling=outside_handling)
    result_shape = size &amp; indices.shape.batch &amp; values.shape.non_spatial
    result_shape = result_shape.without(scatter_dims)
    return NativeTensor(result_, result_shape)</code></pre>
</details>
</dd>
<dt id="phi.math.set_global_precision"><code class="name flex">
<span>def <span class="ident">set_global_precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.</p>
<p>If <code>floating_point_bits</code> is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
Operations may also convert floating point values to this precision, even if the input had a different precision.</p>
<p>If <code>floating_point_bits</code> is None, new tensors will default to float32 unless specified otherwise.
The output of math operations has the same precision as its inputs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>one of (16, 32, 64, None)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_global_precision(floating_point_bits: int):
    &#34;&#34;&#34;
    Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.

    If `floating_point_bits` is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
    Operations may also convert floating point values to this precision, even if the input had a different precision.

    If `floating_point_bits` is None, new tensors will default to float32 unless specified otherwise.
    The output of math operations has the same precision as its inputs.

    Args:
      floating_point_bits: one of (16, 32, 64, None)
    &#34;&#34;&#34;
    _PRECISION[0] = floating_point_bits</code></pre>
</details>
</dd>
<dt id="phi.math.shape"><code class="name flex">
<span>def <span class="ident">shape</span></span>(<span>**dims: int) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape from the dimension names and their respective sizes.</p>
<p>Dimension types are inferred from the names according to the following rules:</p>
<ul>
<li>single letter -&gt; spatial dimension</li>
<li>starts with 'vector' -&gt; channel dimension</li>
<li>else -&gt; batch dimension</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>names -&gt; size</dd>
<dt><strong><code>**dims</code></strong></dt>
<dd>int: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shape(**dims: int) -&gt; Shape:
    &#34;&#34;&#34;
    Creates a Shape from the dimension names and their respective sizes.
    
    Dimension types are inferred from the names according to the following rules:
    
    * single letter -&gt; spatial dimension
    * starts with &#39;vector&#39; -&gt; channel dimension
    * else -&gt; batch dimension

    Args:
      dims: names -&gt; size
      **dims: int: 

    Returns:
      Shape

    &#34;&#34;&#34;
    types = []
    for name, size in dims.items():
        types.append(_infer_dim_type_from_name(name))
    return Shape(dims.values(), dims.keys(), types)</code></pre>
</details>
</dd>
<dt id="phi.math.shift"><code class="name flex">
<span>def <span class="ident">shift</span></span>(<span>x: phi.math._tensors.Tensor, offsets: tuple, dims: tuple = None, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, stack_dim: str = 'shift') ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>shift Tensor by a fixed offset and abiding by extrapolation</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Input data</dd>
<dt><strong><code>offsets</code></strong></dt>
<dd>Shift size</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which to shift, defaults to None</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>padding to be performed at the boundary, defaults to extrapolation.BOUNDARY</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>dimensions to be stacked, defaults to 'shift'</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>offsets</code></strong></dt>
<dd>tuple: </dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation or None:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>str or None:
(Default value = 'shift')</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>offset_tensor</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shift(x: Tensor,
          offsets: tuple,
          dims: tuple or None = None,
          padding: Extrapolation or None = extrapolation.BOUNDARY,
          stack_dim: str or None = &#39;shift&#39;) -&gt; list:
    &#34;&#34;&#34;
    shift Tensor by a fixed offset and abiding by extrapolation

    Args:
      x: Input data
      offsets: Shift size
      dims: Dimensions along which to shift, defaults to None
      padding: padding to be performed at the boundary, defaults to extrapolation.BOUNDARY
      stack_dim: dimensions to be stacked, defaults to &#39;shift&#39;
      x: Tensor: 
      offsets: tuple: 
      dims: tuple or None:  (Default value = None)
      padding: Extrapolation or None:  (Default value = extrapolation.BOUNDARY)
      stack_dim: str or None:  (Default value = &#39;shift&#39;)

    Returns:
      list: offset_tensor

    &#34;&#34;&#34;
    if stack_dim is None:
        assert len(dims) == 1
    x = tensor(x)
    dims = dims if dims is not None else x.shape.spatial.names
    pad_lower = max(0, -min(offsets))
    pad_upper = max(0, max(offsets))
    if padding is not None:
        x = math.pad(x, {axis: (pad_lower, pad_upper) for axis in dims}, mode=padding)
    offset_tensors = []
    for offset in offsets:
        components = []
        for dimension in dims:
            slices = {dim: slice(pad_lower + offset, -pad_upper + offset) if dim == dimension else slice(pad_lower, -pad_upper) for dim in dims}
            slices = {dim: slice(sl.start, sl.stop if sl.stop &lt; 0 else None) for dim, sl in slices.items()}  # replace stop=0 by stop=None
            components.append(x[slices])
        offset_tensors.append(channel_stack(components, stack_dim) if stack_dim is not None else components[0])
    return offset_tensors</code></pre>
</details>
</dd>
<dt id="phi.math.sign"><code class="name flex">
<span>def <span class="ident">sign</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sign(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.sign)</code></pre>
</details>
</dd>
<dt id="phi.math.sin"><code class="name flex">
<span>def <span class="ident">sin</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sin(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.sin)</code></pre>
</details>
</dd>
<dt id="phi.math.solve"><code class="name flex">
<span>def <span class="ident">solve</span></span>(<span>operator, y: phi.math._tensors.Tensor, x0: phi.math._tensors.Tensor, solve_params: phi.math.backend._optim.Solve, callback=None) ‑> Tuple[phi.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>operator</code></strong></dt>
<dd>Function <code>operator(x)</code> or matrix</dd>
<dt><strong><code>y</code></strong></dt>
<dd>Desired output of <code>operator · x</code></dd>
<dt><strong><code>x0</code></strong></dt>
<dd>Initial guess for <code>x</code></dd>
<dt><strong><code>solve_params</code></strong></dt>
<dd>Specifies solver type and parameters such as desired accuracy and maximum iterations.</dd>
<dt><strong><code>callback</code></strong></dt>
<dd>Function to be called after each iteration as <code>callback(x_n)</code>. <em>This argument may be ignored by some backends.</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>converged</code></dt>
<dd>scalar bool tensor representing whether the solve found a solution within the specified accuracy within the allowed iterations</dd>
<dt><code>x</code></dt>
<dd>solution of the linear system of equations <code>operator · x = y</code>.</dd>
<dt><code>iterations</code></dt>
<dd>number of iterations performed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve(operator, y: Tensor, x0: Tensor, solve_params: Solve, callback=None) -&gt; Tuple[Tensor]:
    &#34;&#34;&#34;

    Args:
        operator: Function `operator(x)` or matrix
        y: Desired output of `operator · x`
        x0: Initial guess for `x`
        solve_params: Specifies solver type and parameters such as desired accuracy and maximum iterations.
        callback: Function to be called after each iteration as `callback(x_n)`. *This argument may be ignored by some backends.*

    Returns:
        converged: scalar bool tensor representing whether the solve found a solution within the specified accuracy within the allowed iterations
        x: solution of the linear system of equations `operator · x = y`.
        iterations: number of iterations performed
    &#34;&#34;&#34;
    if not isinstance(solve_params, LinearSolve):
        raise NotImplementedError(&#34;Only linear solve is currently supported. Pass a LinearSolve object&#34;)
    if solve_params.solver not in (None, &#39;CG&#39;):
        raise NotImplementedError(&#34;Only &#39;CG&#39; solver currently supported&#34;)

    from ._track import lin_placeholder, ShiftLinOp
    x0, y = tensors(x0, y)
    batch = (y.shape &amp; x0.shape).batch
    backend = choose_backend(x0.native(), y.native())
    x0_native = backend.reshape(x0.native(), (x0.shape.batch.volume, x0.shape.non_batch.volume))
    y_native = backend.reshape(y.native(), (y.shape.batch.volume, y.shape.non_batch.volume))
    if callable(operator):
        operator_or_matrix = None
        if solve_params.solver_arguments[&#39;bake&#39;] == &#39;sparse&#39;:
            track_time = time.perf_counter()
            x_track = lin_placeholder(x0)
            Ax_track = operator(x_track)
            assert isinstance(Ax_track, ShiftLinOp), &#39;Baking sparse matrix failed. Make sure only supported linear operations are used.&#39;
            track_time = time.perf_counter() - track_time
            build_time = time.perf_counter()
            operator_or_matrix = Ax_track.build_sparse_coordinate_matrix()
            # TODO reshape x0, y so that independent dimensions are batch
            build_time = time.perf_counter() - build_time
            # print_(tensor(operator_or_matrix.todense(), names=&#39;x,y&#39;))
        if operator_or_matrix is None:
            def operator_or_matrix(native_x):
                native_x_shaped = backend.reshape(native_x, x0.shape.non_batch.sizes)
                x = NativeTensor(native_x_shaped, x0.shape.non_batch)
                Ax = operator(x)
                Ax_native = backend.reshape(Ax.native(), backend.shape(native_x))
                return Ax_native
    else:
        operator_or_matrix = backend.reshape(operator.native(), (y.shape.non_batch.volume, x0.shape.non_batch.volume))

    loop_time = time.perf_counter()
    converged, x, iterations = backend.conjugate_gradient(operator_or_matrix, y_native, x0_native, solve_params.relative_tolerance, solve_params.absolute_tolerance, solve_params.max_iterations, &#39;implicit&#39;, callback)
    loop_time = time.perf_counter() - loop_time
    if get_current_profile():
        info = &#34;  \tProfile with trace=False to get more accurate results.&#34; if get_current_profile()._trace else &#34;&#34;
        get_current_profile().add_external_message(f&#34;CG   track: {round(track_time * 1000)} ms  \tbuild: {round(build_time * 1000)} ms  \tloop: {round(loop_time * 1000)} ms / {iterations} iterations {info}&#34;)
    x = backend.reshape(x, batch.sizes + x0.shape.non_batch.sizes)
    return NativeTensor(converged, EMPTY_SHAPE), NativeTensor(x, batch.combined(x0.shape.non_batch)), NativeTensor(iterations, EMPTY_SHAPE)</code></pre>
</details>
</dd>
<dt id="phi.math.sparse_tensor"><code class="name flex">
<span>def <span class="ident">sparse_tensor</span></span>(<span>indices, values, shape)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sparse_tensor(indices, values, shape):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_pad"><code class="name flex">
<span>def <span class="ident">spatial_pad</span></span>(<span>value, pad_width: tuple, mode: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a>) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_pad(value, pad_width: tuple or list, mode: &#39;extrapolation.Extrapolation&#39;) -&gt; Tensor:
    value = tensor(value)
    return pad(value, {n: w for n, w in zip(value.shape.spatial.names, pad_width)}, mode=mode)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_shape"><code class="name flex">
<span>def <span class="ident">spatial_shape</span></span>(<span>sizes: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a> or dict or tuple or list, names: tuple or list = None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape with the following properties:</p>
<ul>
<li>All dimensions are of type 'spatial'</li>
<li>The shape's <code>names</code> match <code>names</code>, if provided</li>
</ul>
<p>Depending on the type of <code>sizes</code>, returns</p>
<ul>
<li>Shape -&gt; (reordered) spatial sub-shape</li>
<li>dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes</li>
<li>tuple/list of sizes -&gt; matches names to sizes and keeps order</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>list of integers or dict or Shape</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Order of dimensions. Optional if isinstance(sizes, (dict, Shape))</dd>
<dt><strong><code>sizes</code></strong></dt>
<dd>Shape or dict or tuple or list: </dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_shape(sizes: Shape or dict or tuple or list, names: tuple or list = None) -&gt; Shape:
    &#34;&#34;&#34;
    Creates a Shape with the following properties:
    
    * All dimensions are of type &#39;spatial&#39;
    * The shape&#39;s `names` match `names`, if provided
    
    Depending on the type of `sizes`, returns
    
    * Shape -&gt; (reordered) spatial sub-shape
    * dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes
    * tuple/list of sizes -&gt; matches names to sizes and keeps order

    Args:
      sizes: list of integers or dict or Shape
      names: Order of dimensions. Optional if isinstance(sizes, (dict, Shape))
      sizes: Shape or dict or tuple or list: 
      names: tuple or list:  (Default value = None)

    Returns:
      Shape containing only spatial dimensions

    &#34;&#34;&#34;
    return _pure_shape(sizes, names, SPATIAL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_stack"><code class="name flex">
<span>def <span class="ident">spatial_stack</span></span>(<span>values, dim: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_stack(values, dim: str):
    return _stack(values, dim, SPATIAL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_sum"><code class="name flex">
<span>def <span class="ident">spatial_sum</span></span>(<span>value: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_sum(value: Tensor):
    return math.sum_(value, dim=value.shape.spatial.names)</code></pre>
</details>
</dd>
<dt id="phi.math.sqrt"><code class="name flex">
<span>def <span class="ident">sqrt</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sqrt(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.sqrt)</code></pre>
</details>
</dd>
<dt id="phi.math.std"><code class="name flex">
<span>def <span class="ident">std</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def std(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.std(native, dim),
                   collapsed_function=lambda inner, red_shape: inner,
                   unaffected_function=lambda value: value * 0)</code></pre>
</details>
</dd>
<dt id="phi.math.sum"><code class="name flex">
<span>def <span class="ident">sum</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sum_(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.sum(native, dim),
                   collapsed_function=lambda inner, red_shape: inner * red_shape.volume)</code></pre>
</details>
</dd>
<dt id="phi.math.tensor"><code class="name flex">
<span>def <span class="ident">tensor</span></span>(<span>data: phi.math._tensors.Tensor, names: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Create a Tensor from the specified data.
<code>data</code> must be one of the following:</p>
<ul>
<li>Number: returns a dimensionless Tensor</li>
<li>Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor</li>
<li>tuple or list of numbers: backs the Tensor with a NumPy array</li>
<li>Tensor: renames dimensions and dimension types if <code>names</code> is specified, otherwise returns the tensor.</li>
<li>Shape: creates a 1D tensor listing the dimension sizes</li>
</ul>
<p>While specifying <code>names</code> is optional in some cases, it is recommended to always specify them.</p>
<p>Dimension types are always inferred from the dimension names if specified.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>native tensor, scalar, sequence, Shape or Tensor</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Dimension names. Dimension types are inferred from the names.</dd>
</dl>
<p>:raise: AssertionError if dimension names are not provided and cannot automatically be inferred
data: Tensor or Shape or tuple or list or numbers.Number:
names: str or tuple or list:
(Default value = None)</p>
<h2 id="returns">Returns</h2>
<p>Tensor containing same values as data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tensor(data: Tensor or Shape or tuple or list or numbers.Number,
           names: str or tuple or list = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Create a Tensor from the specified data.
    `data` must be one of the following:
    
    * Number: returns a dimensionless Tensor
    * Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor
    * tuple or list of numbers: backs the Tensor with a NumPy array
    * Tensor: renames dimensions and dimension types if `names` is specified, otherwise returns the tensor.
    * Shape: creates a 1D tensor listing the dimension sizes
    
    While specifying `names` is optional in some cases, it is recommended to always specify them.
    
    Dimension types are always inferred from the dimension names if specified.

    Args:
      data: native tensor, scalar, sequence, Shape or Tensor
      names: Dimension names. Dimension types are inferred from the names.
    :raise: AssertionError if dimension names are not provided and cannot automatically be inferred
      data: Tensor or Shape or tuple or list or numbers.Number: 
      names: str or tuple or list:  (Default value = None)

    Returns:
      Tensor containing same values as data

    &#34;&#34;&#34;
    if isinstance(data, Tensor):
        if names is None:
            return data
        else:
            names = _shape.parse_dim_names(names, data.rank)
            names = [n if n is not None else o for n, o in zip(names, data.shape.names)]
            types = [_shape._infer_dim_type_from_name(n) if n is not None else o for n, o in zip(names, data.shape.types)]
            new_shape = Shape(data.shape.sizes, names, types)
            return data._with_shape_replaced(new_shape)
    if isinstance(data, (tuple, list)):
        array = np.array(data)
        if array.dtype != np.object:
            data = array
        else:
            elements = tensors(*data, names=None if names is None else names[1:])
            common_shape = _shape.combine_safe(*[e.shape for e in elements])
            rank = 1 + common_shape.rank
            stack_dim = &#39;vector&#39; if names is None else _shape.parse_dim_names(names, rank)[0]
            assert all(stack_dim not in t.shape for t in elements), f&#34;Cannot stack tensors with dimension {stack_dim} because a tensor already has that dimension.&#34;
            elements = [CollapsedTensor(e, common_shape) if e.shape.rank &lt; common_shape.rank else e for e in elements]
            from ._functions import cast_same
            elements = cast_same(*elements)
            return TensorStack(elements, dim_name=stack_dim, dim_type=_shape._infer_dim_type_from_name(stack_dim))
    if isinstance(data, (numbers.Number, str)):
        assert not names
        return NativeTensor(data, EMPTY_SHAPE)
    if isinstance(data, Shape):
        assert names is not None
        return tensor(data.sizes, names)
    backend = choose_backend(data, raise_error=False)
    if backend:
        if names is None:
            assert data.ndim &lt;= 1, &#34;Specify dimension names for tensors with more than 1 dimension&#34;
            names = [&#39;vector&#39;] * backend.ndims(data)  # [] or [&#39;vector&#39;]
            types = [CHANNEL_DIM] * backend.ndims(data)
        else:
            names = _shape.parse_dim_names(names, len(data.shape))
            assert None not in names, f&#34;All names must be specified but got {names}&#34;
            types = [_shape._infer_dim_type_from_name(n) for n in names]
        shape = Shape(data.shape, names, types)
        return NativeTensor(data, shape)
    raise ValueError(f&#34;{type(data)} is not supported. Only (Tensor, tuple, list, np.ndarray, native tensors) are allowed.\nCurrent backends: {BACKENDS}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.tensors"><code class="name flex">
<span>def <span class="ident">tensors</span></span>(<span>*objects, names=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tensors(*objects, names=None):
    return [tensor(obj, names) for obj in objects]</code></pre>
</details>
</dd>
<dt id="phi.math.tile"><code class="name flex">
<span>def <span class="ident">tile</span></span>(<span>value, multiples)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tile(value, multiples):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.to_complex"><code class="name flex">
<span>def <span class="ident">to_complex</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_complex(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.to_complex)</code></pre>
</details>
</dd>
<dt id="phi.math.to_float"><code class="name flex">
<span>def <span class="ident">to_float</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Converts the given tensor to floating point format with the currently specified precision.</p>
<p>The precision can be set globally using <code>math.set_global_precision()</code> and locally using <code>with math.precision()</code>.</p>
<p>See the <code><a title="phi.math" href="#phi.math">phi.math</a></code> module documentation at <a href="https://tum-pbs.github.io/PhiFlow/Math.html">https://tum-pbs.github.io/PhiFlow/Math.html</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>values to convert</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor of same shape as <code>x</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_float(x: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    Converts the given tensor to floating point format with the currently specified precision.
    
    The precision can be set globally using `math.set_global_precision()` and locally using `with math.precision()`.
    
    See the `phi.math` module documentation at https://tum-pbs.github.io/PhiFlow/Math.html

    Args:
      x: values to convert
      x: Tensor: 

    Returns:
      Tensor of same shape as `x`

    &#34;&#34;&#34;
    return _backend_op1(x, Backend.to_float)</code></pre>
</details>
</dd>
<dt id="phi.math.to_int"><code class="name flex">
<span>def <span class="ident">to_int</span></span>(<span>x: phi.math._tensors.Tensor, int64=False) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_int(x: Tensor, int64=False) -&gt; Tensor:
    return x._op1(lambda native: choose_backend(native).to_int(native, int64=int64))</code></pre>
</details>
</dd>
<dt id="phi.math.transpose"><code class="name flex">
<span>def <span class="ident">transpose</span></span>(<span>value, axes)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transpose(value, axes):
    if isinstance(value, Tensor):
        return CollapsedTensor(value, value.shape[axes])
    else:
        return choose_backend(value).transpose(value, axes)</code></pre>
</details>
</dd>
<dt id="phi.math.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>value: phi.math._tensors.Tensor, dim=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(value: Tensor, dim=0):
    assert isinstance(value, Tensor)
    return value.unstack(value.shape.names[dim])</code></pre>
</details>
</dd>
<dt id="phi.math.upsample2x"><code class="name flex">
<span>def <span class="ident">upsample2x</span></span>(<span>grid: phi.math._tensors.Tensor, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Resamples a regular grid to double the number of spatial sample points per dimension.
The grid values at the new points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>half-size grid</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>grid extrapolation</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>dims along which up-sampling is applied. If None, up-sample along all spatial dims.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>double-size grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upsample2x(grid: Tensor,
               padding: Extrapolation = extrapolation.BOUNDARY,
               dims: tuple or None = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Resamples a regular grid to double the number of spatial sample points per dimension.
    The grid values at the new points are determined via linear interpolation.

    Args:
      grid: half-size grid
      padding: grid extrapolation
      dims: dims along which up-sampling is applied. If None, up-sample along all spatial dims.
      grid: Tensor: 
      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)

    Returns:
      double-size grid

    &#34;&#34;&#34;
    for i, dim in enumerate(grid.shape.spatial.only(dims).names):
        left, center, right = shift(grid, (-1, 0, 1), (dim,), padding, None)
        interp_left = 0.25 * left + 0.75 * center
        interp_right = 0.75 * center + 0.25 * right
        stacked = math.spatial_stack([interp_left, interp_right], &#39;_interleave&#39;)
        grid = math.join_dimensions(stacked, (dim, &#39;_interleave&#39;), dim)
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.vec_abs"><code class="name flex">
<span>def <span class="ident">vec_abs</span></span>(<span>vec: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec_abs(vec: Tensor):
    return math.sqrt(math.sum_(vec ** 2, dim=vec.shape.channel.names))</code></pre>
</details>
</dd>
<dt id="phi.math.vec_squared"><code class="name flex">
<span>def <span class="ident">vec_squared</span></span>(<span>vec: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec_squared(vec: Tensor):
    return math.sum_(vec ** 2, dim=vec.shape.channel.names)</code></pre>
</details>
</dd>
<dt id="phi.math.where"><code class="name flex">
<span>def <span class="ident">where</span></span>(<span>condition: phi.math._tensors.Tensor, value_true: phi.math._tensors.Tensor, value_false: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a tensor by choosing either values from <code>value_true</code> or <code>value_false</code> depending on <code>condition</code>.
If <code>condition</code> is not of type boolean, non-zero values are interpreted as True.</p>
<p>This function requires non-None values for <code>value_true</code> and <code>value_false</code>.
To get the indices of True / non-zero values, use :func:<code><a title="phi.math.nonzero" href="#phi.math.nonzero">nonzero()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>condition</code></strong></dt>
<dd>determines where to choose values from value_true or from value_false</dd>
<dt><strong><code>value_true</code></strong></dt>
<dd>values to pick where condition != 0 / True</dd>
<dt><strong><code>value_false</code></strong></dt>
<dd>values to pick where condition == 0 / False</dd>
<dt><strong><code>condition</code></strong></dt>
<dd>Tensor or float or int: </dd>
<dt><strong><code>value_true</code></strong></dt>
<dd>Tensor or float or int: </dd>
<dt><strong><code>value_false</code></strong></dt>
<dd>Tensor or float or int: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor containing dimensions of all inputs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def where(condition: Tensor or float or int, value_true: Tensor or float or int, value_false: Tensor or float or int):
    &#34;&#34;&#34;
    Builds a tensor by choosing either values from `value_true` or `value_false` depending on `condition`.
    If `condition` is not of type boolean, non-zero values are interpreted as True.
    
    This function requires non-None values for `value_true` and `value_false`.
    To get the indices of True / non-zero values, use :func:`nonzero`.

    Args:
      condition: determines where to choose values from value_true or from value_false
      value_true: values to pick where condition != 0 / True
      value_false: values to pick where condition == 0 / False
      condition: Tensor or float or int: 
      value_true: Tensor or float or int: 
      value_false: Tensor or float or int: 

    Returns:
      tensor containing dimensions of all inputs

    &#34;&#34;&#34;
    condition, value_true, value_false = tensors(condition, value_true, value_false)
    shape, (c, vt, vf) = broadcastable_native_tensors(condition, value_true, value_false)
    result = choose_backend(c, vt, vf).where(c, vt, vf)
    return NativeTensor(result, shape)</code></pre>
</details>
</dd>
<dt id="phi.math.with_custom_gradient"><code class="name flex">
<span>def <span class="ident">with_custom_gradient</span></span>(<span>function, inputs, gradient, input_index=0, output_index=None, name_base='custom_gradient_func')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_custom_gradient(function, inputs, gradient, input_index=0, output_index=None, name_base=&#39;custom_gradient_func&#39;):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.zeros"><code class="name flex">
<span>def <span class="ident">zeros</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"><p>Define a tensor with specified shape with value 0 / False everywhere.</p>
<p>This method may not immediately allocate the memory to store the values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong></dt>
<dd>base tensor shape (Default value = EMPTY_SHAPE)</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>data type (Default value = None)</dd>
<dt><strong><code>dimensions</code></strong></dt>
<dd>additional dimensions, types are determined from names</dd>
<dt><strong><code>**dimensions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of specified shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros(shape=EMPTY_SHAPE, dtype=None, **dimensions):
    &#34;&#34;&#34;
    Define a tensor with specified shape with value 0 / False everywhere.
    
    This method may not immediately allocate the memory to store the values.

    Args:
      shape: base tensor shape (Default value = EMPTY_SHAPE)
      dtype: data type (Default value = None)
      dimensions: additional dimensions, types are determined from names
      **dimensions: 

    Returns:
      tensor of specified shape

    &#34;&#34;&#34;
    return _initialize(lambda shape, dtype: CollapsedTensor(NativeTensor(default_backend().zeros((), dtype=dtype), EMPTY_SHAPE), shape), shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.zeros_like"><code class="name flex">
<span>def <span class="ident">zeros_like</span></span>(<span>tensor: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros_like(tensor: Tensor):
    return zeros(tensor.shape, dtype=tensor.dtype)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="phi.math.DType"><code class="flex name class">
<span>class <span class="ident">DType</span></span>
<span>(</span><span>kind: type, bits: int = 8)</span>
</code></dt>
<dd>
<div class="desc"><p>Data type for tensors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kind</code></strong></dt>
<dd>Python type, one of <code>(bool, int, float, complex, str)</code></dd>
<dt><strong><code>bits</code></strong></dt>
<dd>number of bits, typically a multiple of 8.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DType:

    def __init__(self, kind: type, bits: int = 8):
        &#34;&#34;&#34;
        Data type for tensors.

        Args:
          kind: Python type, one of `(bool, int, float, complex, str)`
          bits: number of bits, typically a multiple of 8.
        &#34;&#34;&#34;
        assert kind in (bool, int, float, complex, str)
        if kind is bool:
            assert bits == 8
        else:
            assert isinstance(bits, int)
        self.kind = kind
        &#34;&#34;&#34; Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex) &#34;&#34;&#34;
        self.bits = bits
        &#34;&#34;&#34; Number of bits used to store a single value of this type. See `DType.itemsize`. &#34;&#34;&#34;

    @property
    def precision(self):
        &#34;&#34;&#34; Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. &#34;&#34;&#34;
        if self.kind == float:
            return self.bits
        if self.kind == complex:
            return self.bits // 2
        else:
            return None

    @property
    def itemsize(self):
        &#34;&#34;&#34; Number of bytes used to storea single value of this type. See `DType.bits`. &#34;&#34;&#34;
        assert self.bits % 8 == 0
        return self.bits // 8

    def __eq__(self, other):
        return isinstance(other, DType) and self.kind == other.kind and self.bits == other.bits

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash(self.kind) + hash(self.bits)

    def __repr__(self):
        return f&#34;{self.kind.__name__}{self.bits}&#34;</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.DType.bits"><code class="name">var <span class="ident">bits</span></code></dt>
<dd>
<div class="desc"><p>Number of bits used to store a single value of this type. See <code><a title="phi.math.DType.itemsize" href="#phi.math.DType.itemsize">DType.itemsize</a></code>.</p></div>
</dd>
<dt id="phi.math.DType.itemsize"><code class="name">var <span class="ident">itemsize</span></code></dt>
<dd>
<div class="desc"><p>Number of bytes used to storea single value of this type. See <code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">DType.bits</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def itemsize(self):
    &#34;&#34;&#34; Number of bytes used to storea single value of this type. See `DType.bits`. &#34;&#34;&#34;
    assert self.bits % 8 == 0
    return self.bits // 8</code></pre>
</details>
</dd>
<dt id="phi.math.DType.kind"><code class="name">var <span class="ident">kind</span></code></dt>
<dd>
<div class="desc"><p>Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex)</p></div>
</dd>
<dt id="phi.math.DType.precision"><code class="name">var <span class="ident">precision</span></code></dt>
<dd>
<div class="desc"><p>Floating point precision. Only defined if <code>kind in (float, complex)</code>. For complex values, returns half of <code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">DType.bits</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def precision(self):
    &#34;&#34;&#34; Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. &#34;&#34;&#34;
    if self.kind == float:
        return self.bits
    if self.kind == complex:
        return self.bits // 2
    else:
        return None</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.Extrapolation"><code class="flex name class">
<span>class <span class="ident">Extrapolation</span></span>
<span>(</span><span>pad_rank)</span>
</code></dt>
<dd>
<div class="desc"><p>Extrapolations are used to determine values of grids or other structures outside the sampled bounds.</p>
<p>They play a vital role in padding and sampling.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pad_rank</code></strong></dt>
<dd>low-ranking extrapolations are handled first during mixed-extrapolation padding.</dd>
</dl>
<p>The typical order is periodic=1, boundary=2, symmetric=3, reflect=4, constant=5.</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Extrapolation:

    def __init__(self, pad_rank):
        &#34;&#34;&#34;
        Extrapolations are used to determine values of grids or other structures outside the sampled bounds.

        They play a vital role in padding and sampling.

        Args:
          pad_rank: low-ranking extrapolations are handled first during mixed-extrapolation padding.
        The typical order is periodic=1, boundary=2, symmetric=3, reflect=4, constant=5.

        Returns:

        &#34;&#34;&#34;
        self.pad_rank = pad_rank

    def to_dict(self) -&gt; dict:
        &#34;&#34;&#34;
        Serialize this extrapolation to a dictionary that is serializable (JSON-writable).
        
        Use `from_dict()` to restore the Extrapolation object.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def gradient(self) -&gt; Extrapolation:
        &#34;&#34;&#34;Returns the extrapolation for the spatial gradient of a tensor/field with this extrapolation.&#34;&#34;&#34;
        raise NotImplementedError()

    def pad(self, value: Tensor, widths: dict) -&gt; Tensor:
        &#34;&#34;&#34;
        Pads a tensor using values from self.pad_values()

        Args:
          value: tensor to be padded
          widths: name: str -&gt; (lower: int, upper: int)}
          value: Tensor: 
          widths: dict: 

        Returns:

        &#34;&#34;&#34;
        for dim in widths:
            values = []
            if widths[dim][False] &gt; 0:
                values.append(self.pad_values(value, widths[dim][False], dim, False))
            values.append(value)
            if widths[dim][True] &gt; 0:
                values.append(self.pad_values(value, widths[dim][True], dim, True))
            value = math.concat(values, dim)
        return value

    def pad_values(self, value: Tensor, width: int, dimension: str, upper_edge: bool) -&gt; Tensor:
        &#34;&#34;&#34;
        Determines the values with which the given tensor would be padded at the specified using this extrapolation.

        Args:
          value: tensor to be padded
          width: number of cells to pad perpendicular to the face. Must be larger than zero.
          dimension: axis in which to pad
          upper_edge: True for upper edge, False for lower edge
          value: Tensor: 
          width: int: 
          dimension: str: 
          upper_edge: bool: 

        Returns:
          tensor that can be concatenated to value for padding

        &#34;&#34;&#34;
        raise NotImplementedError()

    def transform_coordinates(self, coordinates: Tensor, shape: Shape) -&gt; Tensor:
        &#34;&#34;&#34;
        If is_copy_pad, transforms outsider coordinates to point to the index from which the value should be copied.
        
        Otherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.
        Coordinates are then snapped to the valid index range.
        This is the default implementation.

        Args:
          coordinates: integer coordinates in index space
          shape: tensor shape
          coordinates: Tensor: 
          shape: Shape: 

        Returns:
          transformed coordinates

        &#34;&#34;&#34;
        return math.clip(coordinates, 0, math.tensor(shape.spatial - 1, &#39;vector&#39;))

    @property
    def is_copy_pad(self):
        &#34;&#34;&#34;:return: True if all pad values are copies of existing values in the tensor to be padded&#34;&#34;&#34;
        return False

    @property
    def native_grid_sample_mode(self) -&gt; Union[str, None]:
        return None

    def __getitem__(self, item):
        return self</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="phi.math.extrapolation.ConstantExtrapolation" href="extrapolation.html#phi.math.extrapolation.ConstantExtrapolation">ConstantExtrapolation</a></li>
<li>phi.math.extrapolation._CopyExtrapolation</li>
<li>phi.math.extrapolation._MixedExtrapolation</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Extrapolation.is_copy_pad"><code class="name">var <span class="ident">is_copy_pad</span></code></dt>
<dd>
<div class="desc"><p>:return: True if all pad values are copies of existing values in the tensor to be padded</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_copy_pad(self):
    &#34;&#34;&#34;:return: True if all pad values are copies of existing values in the tensor to be padded&#34;&#34;&#34;
    return False</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.native_grid_sample_mode"><code class="name">var <span class="ident">native_grid_sample_mode</span> : Union[str, NoneType]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def native_grid_sample_mode(self) -&gt; Union[str, None]:
    return None</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Extrapolation.gradient"><code class="name flex">
<span>def <span class="ident">gradient</span></span>(<span>self) ‑> <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a></span>
</code></dt>
<dd>
<div class="desc"><p>Returns the extrapolation for the spatial gradient of a tensor/field with this extrapolation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gradient(self) -&gt; Extrapolation:
    &#34;&#34;&#34;Returns the extrapolation for the spatial gradient of a tensor/field with this extrapolation.&#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>self, value: <a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a>, widths: dict) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Pads a tensor using values from self.pad_values()</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor to be padded</dd>
<dt><strong><code>widths</code></strong></dt>
<dd>name: str -&gt; (lower: int, upper: int)}</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>widths</code></strong></dt>
<dd>dict: </dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad(self, value: Tensor, widths: dict) -&gt; Tensor:
    &#34;&#34;&#34;
    Pads a tensor using values from self.pad_values()

    Args:
      value: tensor to be padded
      widths: name: str -&gt; (lower: int, upper: int)}
      value: Tensor: 
      widths: dict: 

    Returns:

    &#34;&#34;&#34;
    for dim in widths:
        values = []
        if widths[dim][False] &gt; 0:
            values.append(self.pad_values(value, widths[dim][False], dim, False))
        values.append(value)
        if widths[dim][True] &gt; 0:
            values.append(self.pad_values(value, widths[dim][True], dim, True))
        value = math.concat(values, dim)
    return value</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.pad_values"><code class="name flex">
<span>def <span class="ident">pad_values</span></span>(<span>self, value: <a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a>, width: int, dimension: str, upper_edge: bool) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Determines the values with which the given tensor would be padded at the specified using this extrapolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor to be padded</dd>
<dt><strong><code>width</code></strong></dt>
<dd>number of cells to pad perpendicular to the face. Must be larger than zero.</dd>
<dt><strong><code>dimension</code></strong></dt>
<dd>axis in which to pad</dd>
<dt><strong><code>upper_edge</code></strong></dt>
<dd>True for upper edge, False for lower edge</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>width</code></strong></dt>
<dd>int: </dd>
<dt><strong><code>dimension</code></strong></dt>
<dd>str: </dd>
<dt><strong><code>upper_edge</code></strong></dt>
<dd>bool: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor that can be concatenated to value for padding</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad_values(self, value: Tensor, width: int, dimension: str, upper_edge: bool) -&gt; Tensor:
    &#34;&#34;&#34;
    Determines the values with which the given tensor would be padded at the specified using this extrapolation.

    Args:
      value: tensor to be padded
      width: number of cells to pad perpendicular to the face. Must be larger than zero.
      dimension: axis in which to pad
      upper_edge: True for upper edge, False for lower edge
      value: Tensor: 
      width: int: 
      dimension: str: 
      upper_edge: bool: 

    Returns:
      tensor that can be concatenated to value for padding

    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Serialize this extrapolation to a dictionary that is serializable (JSON-writable).</p>
<p>Use <code>from_dict()</code> to restore the Extrapolation object.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict:
    &#34;&#34;&#34;
    Serialize this extrapolation to a dictionary that is serializable (JSON-writable).
    
    Use `from_dict()` to restore the Extrapolation object.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.transform_coordinates"><code class="name flex">
<span>def <span class="ident">transform_coordinates</span></span>(<span>self, coordinates: <a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a>, shape: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>If is_copy_pad, transforms outsider coordinates to point to the index from which the value should be copied.</p>
<p>Otherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.
Coordinates are then snapped to the valid index range.
This is the default implementation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>coordinates</code></strong></dt>
<dd>integer coordinates in index space</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>tensor shape</dd>
<dt><strong><code>coordinates</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>shape</code></strong></dt>
<dd>Shape: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>transformed coordinates</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_coordinates(self, coordinates: Tensor, shape: Shape) -&gt; Tensor:
    &#34;&#34;&#34;
    If is_copy_pad, transforms outsider coordinates to point to the index from which the value should be copied.
    
    Otherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.
    Coordinates are then snapped to the valid index range.
    This is the default implementation.

    Args:
      coordinates: integer coordinates in index space
      shape: tensor shape
      coordinates: Tensor: 
      shape: Shape: 

    Returns:
      transformed coordinates

    &#34;&#34;&#34;
    return math.clip(coordinates, 0, math.tensor(shape.spatial - 1, &#39;vector&#39;))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.LinearSolve"><code class="flex name class">
<span>class <span class="ident">LinearSolve</span></span>
<span>(</span><span>solver: str = None, relative_tolerance=1e-05, absolute_tolerance=0, max_iterations=1000, bake='sparse', **solver_arguments)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearSolve(Solve):

    def __init__(self,
                 solver: str = None,
                 relative_tolerance=1e-5,
                 absolute_tolerance=0,
                 max_iterations=1000,
                 bake=&#39;sparse&#39;,
                 **solver_arguments):
        Solve.__init__(self, solver, relative_tolerance, absolute_tolerance, max_iterations, bake=bake,
                       **solver_arguments)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>phi.math.backend._optim.Solve</li>
</ul>
</dd>
<dt id="phi.math.Shape"><code class="flex name class">
<span>class <span class="ident">Shape</span></span>
<span>(</span><span>sizes: tuple or list, names: tuple or list, types: tuple or list)</span>
</code></dt>
<dd>
<div class="desc"><p>Shapes enumerate dimensions, each consisting of a name, size and type.</p>
<p>To construct a Shape manually, use <code><a title="phi.math.shape" href="#phi.math.shape">shape()</a></code> instead.
This constructor is meant for internal use only.</p>
<p>Construct a Shape from sizes, names and types sequences.
All arguments must have same length.</p>
<p>To create a Shape with inferred dimension types, use :func:<code>shape(**dims)</code> instead.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>Ordered dimension sizes</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Ordered dimension names, either strings (spatial, batch) or integers (channel)</dd>
<dt><strong><code>types</code></strong></dt>
<dd>Ordered types, all values should be one of (CHANNEL_DIM, SPATIAL_DIM, BATCH_DIM)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Shape:
    &#34;&#34;&#34;Shapes enumerate dimensions, each consisting of a name, size and type.&#34;&#34;&#34;

    def __init__(self, sizes: tuple or list, names: tuple or list, types: tuple or list):
        &#34;&#34;&#34;
        To construct a Shape manually, use `shape()` instead.
        This constructor is meant for internal use only.

        Construct a Shape from sizes, names and types sequences.
        All arguments must have same length.

        To create a Shape with inferred dimension types, use :func:`shape(**dims)` instead.

        Args:
            sizes: Ordered dimension sizes
            names: Ordered dimension names, either strings (spatial, batch) or integers (channel)
            types: Ordered types, all values should be one of (CHANNEL_DIM, SPATIAL_DIM, BATCH_DIM)
        &#34;&#34;&#34;
        assert len(sizes) == len(names) == len(types), f&#34;sizes={sizes} ({len(sizes)}), names={names} ({len(names)}), types={types} ({len(types)})&#34;
        self.sizes = tuple(sizes)
        &#34;&#34;&#34; Ordered dimension sizes as `tuple`  &#34;&#34;&#34;
        self.names = tuple(names)
        &#34;&#34;&#34; Ordered dimension names as `tuple` of `str` &#34;&#34;&#34;
        assert all(isinstance(n, str) for n in names), f&#34;All names must be of type string but got {names}&#34;
        self.types = tuple(types)  # undocumented, may be private

    @property
    def named_sizes(self):
        &#34;&#34;&#34;
        For iterating over names and sizes

            for name, size in shape.named_sizes:

        Returns:
            iterable
        &#34;&#34;&#34;
        return zip(self.names, self.sizes)

    @property
    def spatial_dict(self) -&gt; dict:
        &#34;&#34;&#34; Ordered dictionary mapping dimension names to their respective sizes for all spatial dimensions. &#34;&#34;&#34;
        return {n: s for s, n, t in zip(self.sizes, self.names, self.types) if t == SPATIAL_DIM}

    @property
    def dimensions(self):
        &#34;&#34;&#34;
        For iterating over sizes, names and types.
        Meant for internal use.

        See `Shape.named_sizes()`.
        &#34;&#34;&#34;
        return zip(self.sizes, self.names, self.types)

    def __len__(self):
        return len(self.sizes)

    def __contains__(self, item):
        return item in self.names

    def index(self, name: str or list or tuple or Shape or None):
        &#34;&#34;&#34;
        Finds the index of the dimension(s) within this Shape.

        Args:
          name: dimension name or sequence thereof, including Shape object
          name: str or list or tuple or Shape: 

        Returns:
          single index or sequence of indices

        &#34;&#34;&#34;
        if name is None:
            return None
        if isinstance(name, (list, tuple)):
            return tuple(self.index(n) for n in name)
        if isinstance(name, Shape):
            return tuple(self.index(n) for n in name.names)
        for idx, dim_name in enumerate(self.names):
            if dim_name == name:
                return idx
        raise ValueError(&#34;Shape %s does not contain dimension with name &#39;%s&#39;&#34; % (self, name))

    def indices(self, names: tuple or list or Shape):
        if isinstance(names, (list, tuple)):
            return tuple(self.index(n) for n in names)
        if isinstance(names, Shape):
            return tuple(self.index(n) for n in names.names)
        else:
            raise ValueError(names)

    def get_size(self, dim: str or tuple or list):
        &#34;&#34;&#34;
        Args:
            dim: dimension name or sequence of dimension names

        Returns:
            size associated with `dim`
        &#34;&#34;&#34;
        if isinstance(dim, str):
            return self.sizes[self.names.index(dim)]
        elif isinstance(dim, (tuple, list)):
            return tuple(self.get_size(n) for n in dim)
        else:
            raise ValueError(dim)

    def __getattr__(self, name):
        if name == &#39;names&#39;:
            raise AssertionError(&#34;Attribute missing: %s&#34; % name)
        if name in self.names:
            return self.get_size(name)
        raise AttributeError(&#34;Shape has no attribute &#39;%s&#39;&#34; % (name,))

    def get_type(self, name: str or tuple or list or Shape):
        if isinstance(name, str):
            return self.types[self.names.index(name)]
        elif isinstance(name, (tuple, list)):
            return tuple(self.get_type(n) for n in name)
        elif isinstance(name, Shape):
            return tuple(self.get_type(n) for n in name.names)
        else:
            raise ValueError(name)

    def __getitem__(self, selection):
        if isinstance(selection, int):
            return self.sizes[selection]
        elif isinstance(selection, slice):
            return Shape(self.sizes[selection], self.names[selection], self.types[selection])
        return Shape([self.sizes[i] for i in selection], [self.names[i] for i in selection], [self.types[i] for i in selection])

    @property
    def batch(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the batch dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]

    @property
    def non_batch(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the spatial and channel dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]

    @property
    def spatial(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the spatial dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]

    @property
    def non_spatial(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the batch and channel dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]

    @property
    def channel(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the channel dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]

    @property
    def non_channel(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the batch and spatial dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]

    @property
    def singleton(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size of 1 as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size == 1]]

    @property
    def non_singleton(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size different from 1 as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size != 1]]

    @property
    def zero(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size of 0 as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size == 0]]

    @property
    def non_zero(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size different from 0 as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size != 0]]

    @property
    def undefined(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size of `None` as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size is None]]

    @property
    def defined(self) -&gt; Shape:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size different from `None` as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size is not None]]

    def unstack(self, name=&#39;dims&#39;):
        if name == &#39;dims&#39;:
            return tuple(Shape([self.sizes[i]], [self.names[i]], [self.types[i]]) for i in range(self.rank))
        if name not in self:
            return tuple([self])
        else:
            from ._tensors import Tensor
            inner = self.without(name)
            sizes = []
            dim_size = self.get_size(name)
            for size in inner.sizes:
                if isinstance(size, Tensor) and name in size.shape:
                    sizes.append(size.unstack(name))
                    dim_size = size.shape.get_size(name)
                else:
                    sizes.append(size)
            assert isinstance(dim_size, int)
            shapes = tuple(Shape([int(size[i]) if isinstance(size, tuple) else size for size in sizes], inner.names, inner.types) for i in range(dim_size))
            return shapes

    @property
    def name(self) -&gt; str:
        &#34;&#34;&#34; Only for shapes with a single dimension. Returns the name of the dimension. &#34;&#34;&#34;
        assert self.rank == 1, &#39;Shape.name is only defined for shapes of rank 1.&#39;
        return self.names[0]

    @property
    def is_batch(self) -&gt; bool:
        &#34;&#34;&#34; Tests if all dimensions are of type *batch* &#34;&#34;&#34;
        return all([t == BATCH_DIM for t in self.types])

    @property
    def is_spatial(self) -&gt; bool:
        &#34;&#34;&#34; Tests if all dimensions are of type *spatial* &#34;&#34;&#34;
        return all([t == SPATIAL_DIM for t in self.types])

    @property
    def is_channel(self) -&gt; bool:
        &#34;&#34;&#34; Tests if all dimensions are of type *channel* &#34;&#34;&#34;
        return all([t == CHANNEL_DIM for t in self.types])

    def mask(self, names: tuple or list or set):
        &#34;&#34;&#34;
        Returns a binary sequence corresponding to the names of this Shape.
        A value of 1 means that a dimension of this Shape is contained in `names`.

        Args:
          names: collection of dimension
          names: tuple or list or set: 

        Returns:
          binary sequence

        &#34;&#34;&#34;
        if isinstance(names, str):
            names = [names]
        mask = [1 if name in names else 0 for name in self.names]
        return tuple(mask)

    def __repr__(self):
        strings = [&#39;%s=%s&#39; % (name, size) for size, name, _ in self.dimensions]
        return &#39;(&#39; + &#39;, &#39;.join(strings) + &#39;)&#39;

    def __eq__(self, other):
        if not isinstance(other, Shape):
            return False
        return self.names == other.names and self.types == other.types and self.sizes == other.sizes

    def __ne__(self, other):
        return not self == other

    def normal_order(self):
        sizes = self.batch.sizes + self.spatial.sizes + self.channel.sizes
        names = self.batch.names + self.spatial.names + self.channel.names
        types = self.batch.types + self.spatial.types + self.channel.types
        return Shape(sizes, names, types)

    def reorder(self, names: tuple or list):
        assert len(names) == self.rank
        order = [self.index(n) for n in names]
        return self[order]

    def order_group(self, names: tuple or list or Shape):
        if isinstance(names, Shape):
            names = names.names
        order = []
        for name in self.names:
            if name not in order:
                if name in names:
                    order.extend(names)
                else:
                    order.append(name)
        return order

    def combined(self, other: Shape, combine_spatial=False) -&gt; Shape:
        &#34;&#34;&#34;
        Returns a Shape object that both `self` and `other` can be broadcast to.
        If `self` and `other` are incompatible, raises a ValueError.

        Args:
          other: Shape
          other: Shape: 
          combine_spatial:  (Default value = False)

        Returns:
          combined shape
          :raise: ValueError if shapes don&#39;t match

        &#34;&#34;&#34;
        return combine_safe(self, other, check_exact=[] if combine_spatial else [SPATIAL_DIM])

    def __and__(self, other):
        return combine_safe(self, other, check_exact=[SPATIAL_DIM])

    def expand_batch(self, size, name: str, pos=None) -&gt; Shape:
        return self.expand(size, name, BATCH_DIM, pos)

    def expand_spatial(self, size, name: str, pos=None) -&gt; Shape:
        return self.expand(size, name, SPATIAL_DIM, pos)

    def expand_channel(self, size, name: str, pos=None) -&gt; Shape:
        return self.expand(size, name, CHANNEL_DIM, pos)

    def expand(self, size, name: str, dim_type: str, pos=None) -&gt; Shape:
        &#34;&#34;&#34;
        Add a dimension to the shape.
        
        The resulting shape has linear indices.

        Args:
          size: 
          name: str: 
          dim_type: str: 
          pos:  (Default value = None)

        Returns:

        &#34;&#34;&#34;
        if pos is None:
            same_type_dims = self[[i for i, t in enumerate(self.types) if t == dim_type]]
            if len(same_type_dims) &gt; 0:
                pos = self.index(same_type_dims.names[0])
            else:
                pos = {BATCH_DIM: 0, SPATIAL_DIM: self.batch.rank, CHANNEL_DIM: self.rank + 1}[dim_type]
        elif pos &lt; 0:
            pos += self.rank + 1
        sizes = list(self.sizes)
        names = list(self.names)
        types = list(self.types)
        sizes.insert(pos, size)
        names.insert(pos, name)
        types.insert(pos, dim_type)
        return Shape(sizes, names, types)

    def extend(self, other: Shape, pos=-1) -&gt; Shape:
        if pos == -1:
            return Shape(self.sizes + other.sizes, self.names + other.names, self.types + other.types)
        elif pos == None:
            result = self
            for size, name, dim_type in other.dimensions:
                result = result.expand(size, name, dim_type)
            return result
        else:
            raise NotImplementedError(pos)

    def without(self, dims: str or tuple or list or Shape or None) -&gt; Shape:
        &#34;&#34;&#34;
        Builds a new shape from this one that is missing all given dimensions.
        Dimensions in `dims` that are not part of this Shape are ignored.
        
        The complementary operation is :func:`Shape.only`.

        Args:
          dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
          dims: str or tuple or list or Shape or None: 

        Returns:
          Shape without specified dimensions

        &#34;&#34;&#34;
        if isinstance(dims, str):
            return self[[i for i in range(self.rank) if self.names[i] != dims]]
        if isinstance(dims, (tuple, list)):
            return self[[i for i in range(self.rank) if self.names[i] not in dims]]
        elif isinstance(dims, Shape):
            return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
        elif dims is None:  # subtract all
            return EMPTY_SHAPE
        else:
            raise ValueError(dims)

    reduce = without

    def only(self, dims: str or tuple or list or Shape):
        &#34;&#34;&#34;
        Builds a new shape from this one that only contains the given dimensions.
        Dimensions in `dims` that are not part of this Shape are ignored.
        
        The complementary operation is :func:`Shape.without`.

        Args:
          dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
          dims: str or tuple or list or Shape: 

        Returns:
          Shape containing only specified dimensions

        &#34;&#34;&#34;
        if isinstance(dims, str):
            return self[[i for i in range(self.rank) if self.names[i] == dims]]
        if isinstance(dims, (tuple, list)):
            return self[[i for i in range(self.rank) if self.names[i] in dims]]
        elif isinstance(dims, Shape):
            return self[[i for i in range(self.rank) if self.names[i] in dims.names]]
        elif dims is None:  # keep all
            return self
        else:
            raise ValueError(dims)

    def select(self, *names):
        indices = [self.index(name) for name in names]
        return self[indices]

    @property
    def rank(self) -&gt; int:
        &#34;&#34;&#34;
        Returns the number of dimensions.
        Equal to `len(shape)`.

        See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
        &#34;&#34;&#34;
        return len(self.sizes)

    @property
    def batch_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of batch dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == BATCH_DIM:
                r += 1
        return r

    @property
    def spatial_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == SPATIAL_DIM:
                r += 1
        return r

    @property
    def channel_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of channel dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == CHANNEL_DIM:
                r += 1
        return r

    def to_batch(self, dims: tuple or list or None = None) -&gt; Shape:
        &#34;&#34;&#34;
        Returns a shape like this Shape but with `dims` being of type `batch`.
        
        Leaves this Shape object untouched.

        Args:
          dims: sequence of dimension names to convert or None to convert all dimensions
          dims: tuple or list or None:  (Default value = None)

        Returns:
          new Shape object

        &#34;&#34;&#34;
        if dims is None:
            return Shape(self.sizes, self.names, [BATCH_DIM] * self.rank)
        else:
            return Shape(self.sizes, self.names, [BATCH_DIM if dim in dims else self.types[i] for i, dim in enumerate(self.names)])

    @property
    def well_defined(self):
        &#34;&#34;&#34; Returns True if no dimension is `None`. &#34;&#34;&#34;
        return None not in self.sizes

    @property
    def shape(self, list_dim=&#39;dims&#39;) -&gt; Shape:
        &#34;&#34;&#34;
        Returns the shape of this `Shape`.
        The returned shape will always contain the dimension `list_dim` with a size equal to the `Shape.rank` of this shape.

        Sizes of type `Tensor` can cause the result to have additional dimensions.

        Args:
            list_dim: name of dimension listing the dimensions of this shape

        Returns:
            second order shape
        &#34;&#34;&#34;
        from phi.math import Tensor
        shape = Shape([self.rank], [list_dim], [CHANNEL_DIM])
        for size in self.sizes:
            if isinstance(size, Tensor):
                shape = shape &amp; size.shape
        return shape

    @property
    def is_non_uniform(self) -&gt; bool:
        &#34;&#34;&#34;
        A shape is non-uniform if the size of any dimension varies along another dimension.

        See `Shape.shape`.
        &#34;&#34;&#34;
        from phi.math import Tensor
        for size in self.sizes:
            if isinstance(size, Tensor) and size.rank &gt; 0:
                return True
        return False

    def with_sizes(self, sizes: tuple or list or Shape):
        if isinstance(sizes, Shape):
            sizes = [sizes.get_size(dim) if dim in sizes else self.sizes[i] for i, dim in enumerate(self.names)]
            return Shape(sizes, self.names, self.types)
        else:
            assert len(sizes) == len(self.sizes)
            return Shape(sizes, self.names, self.types)

    def with_size(self, name, size):
        new_sizes = list(self.sizes)
        new_sizes[self.index(name)] = size
        return self.with_sizes(new_sizes)

    def with_names(self, names: str or tuple or list):
        if isinstance(names, str):
            names = parse_dim_names(names, self.rank)
            names = [n if n is not None else o for n, o in zip(names, self.names)]
        return Shape(self.sizes, names, self.types)

    def with_types(self, types: Shape):
        return Shape(self.sizes, self.names, [types.get_type(name) if name in types else self_type for name, self_type in zip(self.names, self.types)])

    def perm(self, names):
        assert set(names) == set(self.names), &#39;names must match existing dimensions %s but got %s&#39; % (self.names, names)
        perm = [self.names.index(name) for name in names]
        return perm

    @property
    def volume(self) -&gt; int or None:
        &#34;&#34;&#34;
        Returns the total number of values contained in a tensor of this shape.
        This is the product of all dimension sizes.

        Returns:
            volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
        &#34;&#34;&#34;
        result = 1
        for size in self.sizes:
            if size is None:
                return None
            result *= size
        return result

    @property
    def is_empty(self) -&gt; bool:
        &#34;&#34;&#34; True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. &#34;&#34;&#34;
        return len(self.sizes) == 0

    def order(self, sequence, default=None) -&gt; Shape:
        &#34;&#34;&#34;
        If sequence is a dict with dimension names as keys, orders its values according to this shape.
        
        Otherwise, the sequence is returned unchanged.

        Args:
          sequence(dict or list or tuple): sequence or dict to be ordered
          default: default value used for dimensions not contained in sequence

        Returns:
          ordered sequence of values
        &#34;&#34;&#34;
        if isinstance(sequence, dict):
            result = [sequence.get(name, default) for name in self.names]
            return result
        if isinstance(sequence, (tuple, list)):
            assert len(sequence) == self.rank
            return sequence
        else:  # just a constant
            return sequence

    def sequence_get(self, sequence, name):
        if isinstance(sequence, dict):
            return sequence[name]
        if isinstance(sequence, (tuple, list)):
            assert len(sequence) == self.rank
            return sequence[self.names.index(name)]
        if math.is_tensor(sequence):
            assert math.staticshape(sequence) == (self.rank,)
            return sequence[self.names.index(name)]
        else:  # just a constant
            return sequence

    def after_pad(self, widths: dict):
        sizes = list(self.sizes)
        for dim, (lo, up) in widths.items():
            sizes[self.index(dim)] += lo + up
        return Shape(sizes, self.names, self.types)

    def after_gather(self, selection: dict):
        result = self
        for name, selection in selection.items():
            if isinstance(selection, int):
                result = result.without(name)
            elif isinstance(selection, slice):
                assert selection.step is None
                start = selection.start or 0
                stop = selection.stop or self.get_size(name)
                if stop &lt; 0:
                    stop += self.get_size(name)
                result = result.with_size(name, stop - start)
            else:
                raise NotImplementedError(f&#34;{type(selection)} not supported. Only (int, slice) allowed.&#34;)
        return result

    def meshgrid(self):
        &#34;&#34;&#34;Builds a sequence containing all multi-indices within a tensor of this shape.&#34;&#34;&#34;
        indices = [0] * self.rank
        while True:
            yield {name: index for name, index in zip(self.names, indices)}
            for i in range(self.rank-1, -1, -1):
                indices[i] = (indices[i] + 1) % self.sizes[i]
                if indices[i] != 0:
                    break
            else:
                return

    product = meshgrid

    def __add__(self, other):
        return self._op1(other, lambda s, o: s + o)

    def __radd__(self, other):
        return self._op1(other, lambda s, o: o + s)

    def __sub__(self, other):
        return self._op1(other, lambda s, o: s - o)

    def __rsub__(self, other):
        return self._op1(other, lambda s, o: o - s)

    def __mul__(self, other):
        return self._op1(other, lambda s, o: s * o)

    def __rmul__(self, other):
        return self._op1(other, lambda s, o: o * s)

    def _op1(self, other, fun):
        if isinstance(other, int):
            return Shape([fun(s, other) for s in self.sizes], self.names, self.types)
        elif isinstance(other, Shape):
            assert self.names == other.names, f&#34;{self.names, other.names}&#34;
            return Shape([fun(s, o) for s, o in zip(self.sizes, other.sizes)], self.names, self.types)
        else:
            return NotImplemented

    def __hash__(self):
        return hash(self.sizes)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Shape.batch"><code class="name">var <span class="ident">batch</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the batch dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the batch dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.batch_rank"><code class="name">var <span class="ident">batch_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of batch dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of batch dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == BATCH_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.channel"><code class="name">var <span class="ident">channel</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the channel dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def channel(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the channel dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.channel_rank"><code class="name">var <span class="ident">channel_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of channel dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def channel_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of channel dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == CHANNEL_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.defined"><code class="name">var <span class="ident">defined</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size different from <code>None</code> as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def defined(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size different from `None` as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size is not None]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.dimensions"><code class="name">var <span class="ident">dimensions</span></code></dt>
<dd>
<div class="desc"><p>For iterating over sizes, names and types.
Meant for internal use.</p>
<p>See <code><a title="phi.math.Shape.named_sizes" href="#phi.math.Shape.named_sizes">Shape.named_sizes</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dimensions(self):
    &#34;&#34;&#34;
    For iterating over sizes, names and types.
    Meant for internal use.

    See `Shape.named_sizes()`.
    &#34;&#34;&#34;
    return zip(self.sizes, self.names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_batch"><code class="name">var <span class="ident">is_batch</span> : bool</code></dt>
<dd>
<div class="desc"><p>Tests if all dimensions are of type <em>batch</em></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_batch(self) -&gt; bool:
    &#34;&#34;&#34; Tests if all dimensions are of type *batch* &#34;&#34;&#34;
    return all([t == BATCH_DIM for t in self.types])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_channel"><code class="name">var <span class="ident">is_channel</span> : bool</code></dt>
<dd>
<div class="desc"><p>Tests if all dimensions are of type <em>channel</em></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_channel(self) -&gt; bool:
    &#34;&#34;&#34; Tests if all dimensions are of type *channel* &#34;&#34;&#34;
    return all([t == CHANNEL_DIM for t in self.types])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_empty"><code class="name">var <span class="ident">is_empty</span> : bool</code></dt>
<dd>
<div class="desc"><p>True if this shape has no dimensions. Equivalent to <code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">Shape.rank</a></code> <code>== 0</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_empty(self) -&gt; bool:
    &#34;&#34;&#34; True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. &#34;&#34;&#34;
    return len(self.sizes) == 0</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_non_uniform"><code class="name">var <span class="ident">is_non_uniform</span> : bool</code></dt>
<dd>
<div class="desc"><p>A shape is non-uniform if the size of any dimension varies along another dimension.</p>
<p>See <code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">Shape.shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_non_uniform(self) -&gt; bool:
    &#34;&#34;&#34;
    A shape is non-uniform if the size of any dimension varies along another dimension.

    See `Shape.shape`.
    &#34;&#34;&#34;
    from phi.math import Tensor
    for size in self.sizes:
        if isinstance(size, Tensor) and size.rank &gt; 0:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_spatial"><code class="name">var <span class="ident">is_spatial</span> : bool</code></dt>
<dd>
<div class="desc"><p>Tests if all dimensions are of type <em>spatial</em></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_spatial(self) -&gt; bool:
    &#34;&#34;&#34; Tests if all dimensions are of type *spatial* &#34;&#34;&#34;
    return all([t == SPATIAL_DIM for t in self.types])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"><p>Only for shapes with a single dimension. Returns the name of the dimension.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self) -&gt; str:
    &#34;&#34;&#34; Only for shapes with a single dimension. Returns the name of the dimension. &#34;&#34;&#34;
    assert self.rank == 1, &#39;Shape.name is only defined for shapes of rank 1.&#39;
    return self.names[0]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.named_sizes"><code class="name">var <span class="ident">named_sizes</span></code></dt>
<dd>
<div class="desc"><p>For iterating over names and sizes</p>
<pre><code>for name, size in shape.named_sizes:
</code></pre>
<h2 id="returns">Returns</h2>
<p>iterable</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def named_sizes(self):
    &#34;&#34;&#34;
    For iterating over names and sizes

        for name, size in shape.named_sizes:

    Returns:
        iterable
    &#34;&#34;&#34;
    return zip(self.names, self.sizes)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.names"><code class="name">var <span class="ident">names</span></code></dt>
<dd>
<div class="desc"><p>Ordered dimension names as <code>tuple</code> of <code>str</code></p></div>
</dd>
<dt id="phi.math.Shape.non_batch"><code class="name">var <span class="ident">non_batch</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the spatial and channel dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_batch(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the spatial and channel dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_channel"><code class="name">var <span class="ident">non_channel</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the batch and spatial dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_channel(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the batch and spatial dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_singleton"><code class="name">var <span class="ident">non_singleton</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size different from 1 as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_singleton(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size different from 1 as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size != 1]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_spatial"><code class="name">var <span class="ident">non_spatial</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the batch and channel dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_spatial(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the batch and channel dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_zero"><code class="name">var <span class="ident">non_zero</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size different from 0 as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_zero(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size different from 0 as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size != 0]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.rank"><code class="name">var <span class="ident">rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Returns the number of dimensions.
Equal to <code>len(<a title="phi.math.shape" href="#phi.math.shape">shape()</a>)</code>.</p>
<p>See <code><a title="phi.math.Shape.is_empty" href="#phi.math.Shape.is_empty">Shape.is_empty</a></code>, <code><a title="phi.math.Shape.batch_rank" href="#phi.math.Shape.batch_rank">Shape.batch_rank</a></code>, <code><a title="phi.math.Shape.spatial_rank" href="#phi.math.Shape.spatial_rank">Shape.spatial_rank</a></code>, <code><a title="phi.math.Shape.channel_rank" href="#phi.math.Shape.channel_rank">Shape.channel_rank</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rank(self) -&gt; int:
    &#34;&#34;&#34;
    Returns the number of dimensions.
    Equal to `len(shape)`.

    See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
    &#34;&#34;&#34;
    return len(self.sizes)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.shape"><code class="name">var <span class="ident">shape</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Returns the shape of this <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.
The returned shape will always contain the dimension <code>list_dim</code> with a size equal to the <code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">Shape.rank</a></code> of this shape.</p>
<p>Sizes of type <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> can cause the result to have additional dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>list_dim</code></strong></dt>
<dd>name of dimension listing the dimensions of this shape</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>second order shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self, list_dim=&#39;dims&#39;) -&gt; Shape:
    &#34;&#34;&#34;
    Returns the shape of this `Shape`.
    The returned shape will always contain the dimension `list_dim` with a size equal to the `Shape.rank` of this shape.

    Sizes of type `Tensor` can cause the result to have additional dimensions.

    Args:
        list_dim: name of dimension listing the dimensions of this shape

    Returns:
        second order shape
    &#34;&#34;&#34;
    from phi.math import Tensor
    shape = Shape([self.rank], [list_dim], [CHANNEL_DIM])
    for size in self.sizes:
        if isinstance(size, Tensor):
            shape = shape &amp; size.shape
    return shape</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.singleton"><code class="name">var <span class="ident">singleton</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size of 1 as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def singleton(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size of 1 as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size == 1]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.sizes"><code class="name">var <span class="ident">sizes</span></code></dt>
<dd>
<div class="desc"><p>Ordered dimension sizes as <code>tuple</code></p></div>
</dd>
<dt id="phi.math.Shape.spatial"><code class="name">var <span class="ident">spatial</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the spatial dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the spatial dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.spatial_dict"><code class="name">var <span class="ident">spatial_dict</span> : dict</code></dt>
<dd>
<div class="desc"><p>Ordered dictionary mapping dimension names to their respective sizes for all spatial dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial_dict(self) -&gt; dict:
    &#34;&#34;&#34; Ordered dictionary mapping dimension names to their respective sizes for all spatial dimensions. &#34;&#34;&#34;
    return {n: s for s, n, t in zip(self.sizes, self.names, self.types) if t == SPATIAL_DIM}</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.spatial_rank"><code class="name">var <span class="ident">spatial_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == SPATIAL_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.undefined"><code class="name">var <span class="ident">undefined</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size of <code>None</code> as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def undefined(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size of `None` as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size is None]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.volume"><code class="name">var <span class="ident">volume</span> : int</code></dt>
<dd>
<div class="desc"><p>Returns the total number of values contained in a tensor of this shape.
This is the product of all dimension sizes.</p>
<h2 id="returns">Returns</h2>
<p>volume as <code>int</code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>None</code> if the shape is not <code><a title="phi.math.Shape.well_defined" href="#phi.math.Shape.well_defined">Shape.well_defined</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def volume(self) -&gt; int or None:
    &#34;&#34;&#34;
    Returns the total number of values contained in a tensor of this shape.
    This is the product of all dimension sizes.

    Returns:
        volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
    &#34;&#34;&#34;
    result = 1
    for size in self.sizes:
        if size is None:
            return None
        result *= size
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.well_defined"><code class="name">var <span class="ident">well_defined</span></code></dt>
<dd>
<div class="desc"><p>Returns True if no dimension is <code>None</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def well_defined(self):
    &#34;&#34;&#34; Returns True if no dimension is `None`. &#34;&#34;&#34;
    return None not in self.sizes</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.zero"><code class="name">var <span class="ident">zero</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size of 0 as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def zero(self) -&gt; Shape:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size of 0 as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size == 0]]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Shape.after_gather"><code class="name flex">
<span>def <span class="ident">after_gather</span></span>(<span>self, selection: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def after_gather(self, selection: dict):
    result = self
    for name, selection in selection.items():
        if isinstance(selection, int):
            result = result.without(name)
        elif isinstance(selection, slice):
            assert selection.step is None
            start = selection.start or 0
            stop = selection.stop or self.get_size(name)
            if stop &lt; 0:
                stop += self.get_size(name)
            result = result.with_size(name, stop - start)
        else:
            raise NotImplementedError(f&#34;{type(selection)} not supported. Only (int, slice) allowed.&#34;)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.after_pad"><code class="name flex">
<span>def <span class="ident">after_pad</span></span>(<span>self, widths: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def after_pad(self, widths: dict):
    sizes = list(self.sizes)
    for dim, (lo, up) in widths.items():
        sizes[self.index(dim)] += lo + up
    return Shape(sizes, self.names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.combined"><code class="name flex">
<span>def <span class="ident">combined</span></span>(<span>self, other: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>, combine_spatial=False) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a Shape object that both <code>self</code> and <code>other</code> can be broadcast to.
If <code>self</code> and <code>other</code> are incompatible, raises a ValueError.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>other</code></strong></dt>
<dd>Shape</dd>
<dt><strong><code>other</code></strong></dt>
<dd>Shape: </dd>
<dt><strong><code>combine_spatial</code></strong></dt>
<dd>(Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>combined shape
:raise: ValueError if shapes don't match</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combined(self, other: Shape, combine_spatial=False) -&gt; Shape:
    &#34;&#34;&#34;
    Returns a Shape object that both `self` and `other` can be broadcast to.
    If `self` and `other` are incompatible, raises a ValueError.

    Args:
      other: Shape
      other: Shape: 
      combine_spatial:  (Default value = False)

    Returns:
      combined shape
      :raise: ValueError if shapes don&#39;t match

    &#34;&#34;&#34;
    return combine_safe(self, other, check_exact=[] if combine_spatial else [SPATIAL_DIM])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.expand"><code class="name flex">
<span>def <span class="ident">expand</span></span>(<span>self, size, name: str, dim_type: str, pos=None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Add a dimension to the shape.</p>
<p>The resulting shape has linear indices.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>name</code></strong></dt>
<dd>str: </dd>
<dt><strong><code>dim_type</code></strong></dt>
<dd>str: </dd>
<dt><strong><code>pos</code></strong></dt>
<dd>(Default value = None)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand(self, size, name: str, dim_type: str, pos=None) -&gt; Shape:
    &#34;&#34;&#34;
    Add a dimension to the shape.
    
    The resulting shape has linear indices.

    Args:
      size: 
      name: str: 
      dim_type: str: 
      pos:  (Default value = None)

    Returns:

    &#34;&#34;&#34;
    if pos is None:
        same_type_dims = self[[i for i, t in enumerate(self.types) if t == dim_type]]
        if len(same_type_dims) &gt; 0:
            pos = self.index(same_type_dims.names[0])
        else:
            pos = {BATCH_DIM: 0, SPATIAL_DIM: self.batch.rank, CHANNEL_DIM: self.rank + 1}[dim_type]
    elif pos &lt; 0:
        pos += self.rank + 1
    sizes = list(self.sizes)
    names = list(self.names)
    types = list(self.types)
    sizes.insert(pos, size)
    names.insert(pos, name)
    types.insert(pos, dim_type)
    return Shape(sizes, names, types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.expand_batch"><code class="name flex">
<span>def <span class="ident">expand_batch</span></span>(<span>self, size, name: str, pos=None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_batch(self, size, name: str, pos=None) -&gt; Shape:
    return self.expand(size, name, BATCH_DIM, pos)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.expand_channel"><code class="name flex">
<span>def <span class="ident">expand_channel</span></span>(<span>self, size, name: str, pos=None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_channel(self, size, name: str, pos=None) -&gt; Shape:
    return self.expand(size, name, CHANNEL_DIM, pos)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.expand_spatial"><code class="name flex">
<span>def <span class="ident">expand_spatial</span></span>(<span>self, size, name: str, pos=None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_spatial(self, size, name: str, pos=None) -&gt; Shape:
    return self.expand(size, name, SPATIAL_DIM, pos)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.extend"><code class="name flex">
<span>def <span class="ident">extend</span></span>(<span>self, other: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>, pos=-1) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extend(self, other: Shape, pos=-1) -&gt; Shape:
    if pos == -1:
        return Shape(self.sizes + other.sizes, self.names + other.names, self.types + other.types)
    elif pos == None:
        result = self
        for size, name, dim_type in other.dimensions:
            result = result.expand(size, name, dim_type)
        return result
    else:
        raise NotImplementedError(pos)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_size"><code class="name flex">
<span>def <span class="ident">get_size</span></span>(<span>self, dim: str or tuple or list)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>dimension name or sequence of dimension names</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>size associated with <code>dim</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_size(self, dim: str or tuple or list):
    &#34;&#34;&#34;
    Args:
        dim: dimension name or sequence of dimension names

    Returns:
        size associated with `dim`
    &#34;&#34;&#34;
    if isinstance(dim, str):
        return self.sizes[self.names.index(dim)]
    elif isinstance(dim, (tuple, list)):
        return tuple(self.get_size(n) for n in dim)
    else:
        raise ValueError(dim)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_type"><code class="name flex">
<span>def <span class="ident">get_type</span></span>(<span>self, name: str or tuple or list or <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_type(self, name: str or tuple or list or Shape):
    if isinstance(name, str):
        return self.types[self.names.index(name)]
    elif isinstance(name, (tuple, list)):
        return tuple(self.get_type(n) for n in name)
    elif isinstance(name, Shape):
        return tuple(self.get_type(n) for n in name.names)
    else:
        raise ValueError(name)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.index"><code class="name flex">
<span>def <span class="ident">index</span></span>(<span>self, name: str or list or tuple or <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a> or None)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the index of the dimension(s) within this Shape.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>dimension name or sequence thereof, including Shape object</dd>
<dt><strong><code>name</code></strong></dt>
<dd>str or list or tuple or Shape: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>single index or sequence of indices</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def index(self, name: str or list or tuple or Shape or None):
    &#34;&#34;&#34;
    Finds the index of the dimension(s) within this Shape.

    Args:
      name: dimension name or sequence thereof, including Shape object
      name: str or list or tuple or Shape: 

    Returns:
      single index or sequence of indices

    &#34;&#34;&#34;
    if name is None:
        return None
    if isinstance(name, (list, tuple)):
        return tuple(self.index(n) for n in name)
    if isinstance(name, Shape):
        return tuple(self.index(n) for n in name.names)
    for idx, dim_name in enumerate(self.names):
        if dim_name == name:
            return idx
    raise ValueError(&#34;Shape %s does not contain dimension with name &#39;%s&#39;&#34; % (self, name))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.indices"><code class="name flex">
<span>def <span class="ident">indices</span></span>(<span>self, names: tuple or list or <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def indices(self, names: tuple or list or Shape):
    if isinstance(names, (list, tuple)):
        return tuple(self.index(n) for n in names)
    if isinstance(names, Shape):
        return tuple(self.index(n) for n in names.names)
    else:
        raise ValueError(names)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.mask"><code class="name flex">
<span>def <span class="ident">mask</span></span>(<span>self, names: tuple or list or set)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a binary sequence corresponding to the names of this Shape.
A value of 1 means that a dimension of this Shape is contained in <code>names</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>names</code></strong></dt>
<dd>collection of dimension</dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list or set: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>binary sequence</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mask(self, names: tuple or list or set):
    &#34;&#34;&#34;
    Returns a binary sequence corresponding to the names of this Shape.
    A value of 1 means that a dimension of this Shape is contained in `names`.

    Args:
      names: collection of dimension
      names: tuple or list or set: 

    Returns:
      binary sequence

    &#34;&#34;&#34;
    if isinstance(names, str):
        names = [names]
    mask = [1 if name in names else 0 for name in self.names]
    return tuple(mask)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a sequence containing all multi-indices within a tensor of this shape.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(self):
    &#34;&#34;&#34;Builds a sequence containing all multi-indices within a tensor of this shape.&#34;&#34;&#34;
    indices = [0] * self.rank
    while True:
        yield {name: index for name, index in zip(self.names, indices)}
        for i in range(self.rank-1, -1, -1):
            indices[i] = (indices[i] + 1) % self.sizes[i]
            if indices[i] != 0:
                break
        else:
            return</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.normal_order"><code class="name flex">
<span>def <span class="ident">normal_order</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normal_order(self):
    sizes = self.batch.sizes + self.spatial.sizes + self.channel.sizes
    names = self.batch.names + self.spatial.names + self.channel.names
    types = self.batch.types + self.spatial.types + self.channel.types
    return Shape(sizes, names, types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.only"><code class="name flex">
<span>def <span class="ident">only</span></span>(<span>self, dims: str or tuple or list or <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that only contains the given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is :func:<code><a title="phi.math.Shape.without" href="#phi.math.Shape.without">Shape.without()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>single dimension (str) or collection of dimensions (tuple, list, Shape)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>str or tuple or list or Shape: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only specified dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def only(self, dims: str or tuple or list or Shape):
    &#34;&#34;&#34;
    Builds a new shape from this one that only contains the given dimensions.
    Dimensions in `dims` that are not part of this Shape are ignored.
    
    The complementary operation is :func:`Shape.without`.

    Args:
      dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
      dims: str or tuple or list or Shape: 

    Returns:
      Shape containing only specified dimensions

    &#34;&#34;&#34;
    if isinstance(dims, str):
        return self[[i for i in range(self.rank) if self.names[i] == dims]]
    if isinstance(dims, (tuple, list)):
        return self[[i for i in range(self.rank) if self.names[i] in dims]]
    elif isinstance(dims, Shape):
        return self[[i for i in range(self.rank) if self.names[i] in dims.names]]
    elif dims is None:  # keep all
        return self
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.order"><code class="name flex">
<span>def <span class="ident">order</span></span>(<span>self, sequence, default=None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>If sequence is a dict with dimension names as keys, orders its values according to this shape.</p>
<p>Otherwise, the sequence is returned unchanged.</p>
<h2 id="args">Args</h2>
<dl>
<dt>sequence(dict or list or tuple): sequence or dict to be ordered</dt>
<dt><strong><code>default</code></strong></dt>
<dd>default value used for dimensions not contained in sequence</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ordered sequence of values</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def order(self, sequence, default=None) -&gt; Shape:
    &#34;&#34;&#34;
    If sequence is a dict with dimension names as keys, orders its values according to this shape.
    
    Otherwise, the sequence is returned unchanged.

    Args:
      sequence(dict or list or tuple): sequence or dict to be ordered
      default: default value used for dimensions not contained in sequence

    Returns:
      ordered sequence of values
    &#34;&#34;&#34;
    if isinstance(sequence, dict):
        result = [sequence.get(name, default) for name in self.names]
        return result
    if isinstance(sequence, (tuple, list)):
        assert len(sequence) == self.rank
        return sequence
    else:  # just a constant
        return sequence</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.order_group"><code class="name flex">
<span>def <span class="ident">order_group</span></span>(<span>self, names: tuple or list or <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def order_group(self, names: tuple or list or Shape):
    if isinstance(names, Shape):
        names = names.names
    order = []
    for name in self.names:
        if name not in order:
            if name in names:
                order.extend(names)
            else:
                order.append(name)
    return order</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.perm"><code class="name flex">
<span>def <span class="ident">perm</span></span>(<span>self, names)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def perm(self, names):
    assert set(names) == set(self.names), &#39;names must match existing dimensions %s but got %s&#39; % (self.names, names)
    perm = [self.names.index(name) for name in names]
    return perm</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.product"><code class="name flex">
<span>def <span class="ident">product</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a sequence containing all multi-indices within a tensor of this shape.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(self):
    &#34;&#34;&#34;Builds a sequence containing all multi-indices within a tensor of this shape.&#34;&#34;&#34;
    indices = [0] * self.rank
    while True:
        yield {name: index for name, index in zip(self.names, indices)}
        for i in range(self.rank-1, -1, -1):
            indices[i] = (indices[i] + 1) % self.sizes[i]
            if indices[i] != 0:
                break
        else:
            return</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.reduce"><code class="name flex">
<span>def <span class="ident">reduce</span></span>(<span>self, dims: str or tuple or list or <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a> or None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that is missing all given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is :func:<code><a title="phi.math.Shape.only" href="#phi.math.Shape.only">Shape.only()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>single dimension (str) or collection of dimensions (tuple, list, Shape)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>str or tuple or list or Shape or None: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape without specified dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def without(self, dims: str or tuple or list or Shape or None) -&gt; Shape:
    &#34;&#34;&#34;
    Builds a new shape from this one that is missing all given dimensions.
    Dimensions in `dims` that are not part of this Shape are ignored.
    
    The complementary operation is :func:`Shape.only`.

    Args:
      dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
      dims: str or tuple or list or Shape or None: 

    Returns:
      Shape without specified dimensions

    &#34;&#34;&#34;
    if isinstance(dims, str):
        return self[[i for i in range(self.rank) if self.names[i] != dims]]
    if isinstance(dims, (tuple, list)):
        return self[[i for i in range(self.rank) if self.names[i] not in dims]]
    elif isinstance(dims, Shape):
        return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
    elif dims is None:  # subtract all
        return EMPTY_SHAPE
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.reorder"><code class="name flex">
<span>def <span class="ident">reorder</span></span>(<span>self, names: tuple or list)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reorder(self, names: tuple or list):
    assert len(names) == self.rank
    order = [self.index(n) for n in names]
    return self[order]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.select"><code class="name flex">
<span>def <span class="ident">select</span></span>(<span>self, *names)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select(self, *names):
    indices = [self.index(name) for name in names]
    return self[indices]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.sequence_get"><code class="name flex">
<span>def <span class="ident">sequence_get</span></span>(<span>self, sequence, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sequence_get(self, sequence, name):
    if isinstance(sequence, dict):
        return sequence[name]
    if isinstance(sequence, (tuple, list)):
        assert len(sequence) == self.rank
        return sequence[self.names.index(name)]
    if math.is_tensor(sequence):
        assert math.staticshape(sequence) == (self.rank,)
        return sequence[self.names.index(name)]
    else:  # just a constant
        return sequence</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.to_batch"><code class="name flex">
<span>def <span class="ident">to_batch</span></span>(<span>self, dims: tuple or list or None = None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a shape like this Shape but with <code>dims</code> being of type <code>batch</code>.</p>
<p>Leaves this Shape object untouched.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>sequence of dimension names to convert or None to convert all dimensions</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or list or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>new Shape object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_batch(self, dims: tuple or list or None = None) -&gt; Shape:
    &#34;&#34;&#34;
    Returns a shape like this Shape but with `dims` being of type `batch`.
    
    Leaves this Shape object untouched.

    Args:
      dims: sequence of dimension names to convert or None to convert all dimensions
      dims: tuple or list or None:  (Default value = None)

    Returns:
      new Shape object

    &#34;&#34;&#34;
    if dims is None:
        return Shape(self.sizes, self.names, [BATCH_DIM] * self.rank)
    else:
        return Shape(self.sizes, self.names, [BATCH_DIM if dim in dims else self.types[i] for i, dim in enumerate(self.names)])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, name='dims')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(self, name=&#39;dims&#39;):
    if name == &#39;dims&#39;:
        return tuple(Shape([self.sizes[i]], [self.names[i]], [self.types[i]]) for i in range(self.rank))
    if name not in self:
        return tuple([self])
    else:
        from ._tensors import Tensor
        inner = self.without(name)
        sizes = []
        dim_size = self.get_size(name)
        for size in inner.sizes:
            if isinstance(size, Tensor) and name in size.shape:
                sizes.append(size.unstack(name))
                dim_size = size.shape.get_size(name)
            else:
                sizes.append(size)
        assert isinstance(dim_size, int)
        shapes = tuple(Shape([int(size[i]) if isinstance(size, tuple) else size for size in sizes], inner.names, inner.types) for i in range(dim_size))
        return shapes</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_names"><code class="name flex">
<span>def <span class="ident">with_names</span></span>(<span>self, names: str or tuple or list)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_names(self, names: str or tuple or list):
    if isinstance(names, str):
        names = parse_dim_names(names, self.rank)
        names = [n if n is not None else o for n, o in zip(names, self.names)]
    return Shape(self.sizes, names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_size"><code class="name flex">
<span>def <span class="ident">with_size</span></span>(<span>self, name, size)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_size(self, name, size):
    new_sizes = list(self.sizes)
    new_sizes[self.index(name)] = size
    return self.with_sizes(new_sizes)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_sizes"><code class="name flex">
<span>def <span class="ident">with_sizes</span></span>(<span>self, sizes: tuple or list or <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_sizes(self, sizes: tuple or list or Shape):
    if isinstance(sizes, Shape):
        sizes = [sizes.get_size(dim) if dim in sizes else self.sizes[i] for i, dim in enumerate(self.names)]
        return Shape(sizes, self.names, self.types)
    else:
        assert len(sizes) == len(self.sizes)
        return Shape(sizes, self.names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_types"><code class="name flex">
<span>def <span class="ident">with_types</span></span>(<span>self, types: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_types(self, types: Shape):
    return Shape(self.sizes, self.names, [types.get_type(name) if name in types else self_type for name, self_type in zip(self.names, self.types)])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.without"><code class="name flex">
<span>def <span class="ident">without</span></span>(<span>self, dims: str or tuple or list or <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a> or None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that is missing all given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is :func:<code><a title="phi.math.Shape.only" href="#phi.math.Shape.only">Shape.only()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>single dimension (str) or collection of dimensions (tuple, list, Shape)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>str or tuple or list or Shape or None: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape without specified dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def without(self, dims: str or tuple or list or Shape or None) -&gt; Shape:
    &#34;&#34;&#34;
    Builds a new shape from this one that is missing all given dimensions.
    Dimensions in `dims` that are not part of this Shape are ignored.
    
    The complementary operation is :func:`Shape.only`.

    Args:
      dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
      dims: str or tuple or list or Shape or None: 

    Returns:
      Shape without specified dimensions

    &#34;&#34;&#34;
    if isinstance(dims, str):
        return self[[i for i in range(self.rank) if self.names[i] != dims]]
    if isinstance(dims, (tuple, list)):
        return self[[i for i in range(self.rank) if self.names[i] not in dims]]
    elif isinstance(dims, Shape):
        return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
    elif dims is None:  # subtract all
        return EMPTY_SHAPE
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.Solve"><code class="flex name class">
<span>class <span class="ident">Solve</span></span>
<span>(</span><span>solver: str = None, relative_tolerance=1e-05, absolute_tolerance=0, max_iterations=1000, **solver_arguments)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Solve:

    def __init__(self,
                 solver: str = None,
                 relative_tolerance=1e-5,
                 absolute_tolerance=0,
                 max_iterations=1000,
                 **solver_arguments):
        self.solver = solver
        self.relative_tolerance = relative_tolerance
        self.absolute_tolerance = absolute_tolerance
        self.max_iterations = max_iterations
        self.solver_arguments = solver_arguments</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li>phi.math.backend._optim.LinearSolve</li>
</ul>
</dd>
<dt id="phi.math.Tensor"><code class="flex name class">
<span>class <span class="ident">Tensor</span></span>
</code></dt>
<dd>
<div class="desc"><p>Tensors with grouped and named dimensions.</p>
<p>All tensors are editable.</p>
<p>The internal data representation of a tensor can change, even without being edited.</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tensor:
    &#34;&#34;&#34;
    Tensors with grouped and named dimensions.
    
    All tensors are editable.
    
    The internal data representation of a tensor can change, even without being edited.

    Args:

    Returns:

    &#34;&#34;&#34;

    def native(self, order: str or tuple or list = None):
        &#34;&#34;&#34;
        Returns a native tensor object with the dimensions ordered according to `order`.
        
        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
        
        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

        Args:
          order: optional) list of dimension names. If not given, the current order is kept.
          order: str or tuple or list:  (Default value = None)

        Returns:
          native tensor object
          :raise: ValueError if the tensor cannot be transposed to match target_shape

        &#34;&#34;&#34;
        raise NotImplementedError()

    def numpy(self, order: str or tuple or list = None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns this tensor as a NumPy ndarray object with dimensions ordered according to `order`.
        
        *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
        To get a differentiable tensor, use :func:`Tensor.native` instead.
        
        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
        
        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

        Args:
          order: optional) list of dimension names. If not given, the current order is kept.
          order: str or tuple or list:  (Default value = None)

        Returns:
          NumPy representation
          :raise: ValueError if the tensor cannot be transposed to match target_shape

        &#34;&#34;&#34;
        native = self.native(order=order)
        return choose_backend(native).numpy(native)

    @property
    def dtype(self):
        raise NotImplementedError()

    @property
    def shape(self) -&gt; Shape:
        raise NotImplementedError()

    def _with_shape_replaced(self, new_shape):
        raise NotImplementedError()

    @property
    def ndims(self) -&gt; int:
        return self.shape.rank

    @property
    def rank(self) -&gt; int:
        return self.shape.rank

    @property
    def _is_special(self) -&gt; bool:
        &#34;&#34;&#34;
        Special tensors store additional internal information.
        They should not be converted to native() in intermediate operations.
        
        Tracking tensors are special tensors.
        
        TensorStack prevents performing the actual stack operation if one of its component tensors is special.

        Args:

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError()

    def __len__(self):
        return self.shape.volume if self.rank == 1 else NotImplemented

    def __bool__(self):
        if self.rank == 0:
            return bool(self.native())
        else:
            from phi.math._functions import all_
            return bool(all_(self))

    def __int__(self):
        return int(self.native()) if self.rank == 0 else NotImplemented

    def __float__(self):
        return float(self.native()) if self.rank == 0 else NotImplemented

    def __complex__(self):
        return complex(self.native()) if self.rank == 0 else NotImplemented

    def __index__(self):
        return int(self.native()) if self.rank == 0 and np.issubdtype(self.dtype, int) else NotImplemented

    def _summary_str(self) -&gt; str:
        from ._functions import all_available, min_, max_
        if all_available(self):
            if self.rank == 0:
                return str(self.numpy())
            elif self.shape.volume is not None and self.shape.volume &lt;= 6:
                content = list(np.reshape(self.numpy(), [-1]))
                content = &#39;, &#39;.join([repr(number) for number in content])
                if self.shape.rank == 1 and (self.dtype.kind in (bool, int) or self.dtype.precision == get_precision()):
                    return f&#34;({content}) along {self.shape.name}&#34;
                return f&#34;{self.shape} {self.dtype}  {content}&#34;
            else:
                min_val, max_val = min_(self), max_(self)
                return f&#34;{self.shape} {self.dtype}  {min_val} &lt; ... &lt; {max_val}&#34;
        else:
            if self.rank == 0:
                return f&#34;scalar {self.dtype}&#34;
            else:
                return f&#34;{self.shape} {self.dtype}&#34;

    def __repr__(self):
        return self._summary_str()

    def __getitem__(self, item):
        if isinstance(item, Tensor):
            from ._functions import gather
            return gather(self, item)
        if isinstance(item, (int, slice)):
            assert self.rank == 1
            item = {self.shape.names[0]: item}
        if isinstance(item, (tuple, list)):
            if item[0] == Ellipsis:
                assert len(item) - 1 == self.shape.channel.rank
                item = {name: selection for name, selection in zip(self.shape.channel.names, item[1:])}
            elif len(item) == self.shape.channel.rank:
                item = {name: selection for name, selection in zip(self.shape.channel.names, item)}
            elif len(item) == self.shape.rank:  # legacy indexing
                warnings.warn(&#34;Slicing with sequence should only be used for channel dimensions.&#34;)
                item = {name: selection for name, selection in zip(self.shape.names, item)}
        assert isinstance(item, dict)  # dict mapping name -&gt; slice/int
        return self._getitem(item)

    def _getitem(self, selection: dict) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Slice the tensor along specified dimensions.

        Args:
          selection: dim_name: str -&gt; int or slice
          selection: dict: 

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError()

    def flip(self, *dims: str) -&gt; &#39;Tensor&#39;:
        raise NotImplementedError()

    # def __setitem__(self, key, value):
    #     &#34;&#34;&#34;
    #     All tensors are editable.
    #
    #     :param key: list/tuple of slices / indices
    #     :param value:
    #     :return:
    #     &#34;&#34;&#34;
    #     raise NotImplementedError()

    def unstack(self, dimension):
        &#34;&#34;&#34;
        Splits this tensor along the specified dimension.
        The returned tensors have the same dimensions as this tensor save the unstacked dimension.

        Args:
          dimension(str or int or _TensorDim): name of dimension or Dimension or None for component dimension

        Returns:
          tuple of tensors

        &#34;&#34;&#34;
        raise NotImplementedError()

    def dimension(self, name):
        return _TensorDim(self, name)

    def __getattr__(self, name):
        if name.startswith(&#39;_&#39;):
            raise AttributeError(f&#34;&#39;{type(self)}&#39; object has no attribute &#39;{name}&#39;&#34;)
        assert name not in (&#39;shape&#39;, &#39;_shape&#39;)
        return _TensorDim(self, name)

    def __add__(self, other):
        return self._op2(other, lambda x, y: x + y, lambda x, y: choose_backend(x, y).add(x, y))

    def __radd__(self, other):
        return self._op2(other, lambda x, y: y + x, lambda x, y: choose_backend(x, y).add(y, x))

    def __sub__(self, other):
        return self._op2(other, lambda x, y: x - y, lambda x, y: choose_backend(x, y).sub(x, y))

    def __rsub__(self, other):
        return self._op2(other, lambda x, y: y - x, lambda x, y: choose_backend(x, y).sub(y, x))

    def __and__(self, other):
        return self._op2(other, lambda x, y: x &amp; y, lambda x, y: x &amp; y)

    def __or__(self, other):
        return self._op2(other, lambda x, y: x | y, lambda x, y: x | y)

    def __xor__(self, other):
        return self._op2(other, lambda x, y: x ^ y, lambda x, y: x ^ y)

    def __mul__(self, other):
        return self._op2(other, lambda x, y: x * y, lambda x, y: choose_backend(x, y).mul(x, y))

    def __rmul__(self, other):
        return self._op2(other, lambda x, y: y * x, lambda x, y: choose_backend(x, y).mul(y, x))

    def __truediv__(self, other):
        return self._op2(other, lambda x, y: x / y, lambda x, y: choose_backend(x, y).div(x, y))

    def __rtruediv__(self, other):
        return self._op2(other, lambda x, y: y / x, lambda x, y: choose_backend(x, y).div(y, x))

    def __divmod__(self, other):
        return self._op2(other, lambda x, y: divmod(x, y), lambda x, y: divmod(x, y))

    def __rdivmod__(self, other):
        return self._op2(other, lambda x, y: divmod(y, x), lambda x, y: divmod(y, x))

    def __floordiv__(self, other):
        return self._op2(other, lambda x, y: x // y, lambda x, y: x // y)

    def __rfloordiv__(self, other):
        return self._op2(other, lambda x, y: y // x, lambda x, y: y // x)

    def __pow__(self, power, modulo=None):
        assert modulo is None
        return self._op2(power, lambda x, y: x ** y, lambda x, y: choose_backend(x, y).pow(x, y))

    def __rpow__(self, other):
        return self._op2(other, lambda x, y: y ** x, lambda x, y: choose_backend(x, y).pow(y, x))

    def __mod__(self, other):
        return self._op2(other, lambda x, y: x % y, lambda x, y: choose_backend(x, y).mod(x, y))

    def __rmod__(self, other):
        return self._op2(other, lambda x, y: y % x, lambda x, y: choose_backend(x, y).mod(y, x))

    def __eq__(self, other):
        return self._op2(other, lambda x, y: x == y, lambda x, y: choose_backend(x, y).equal(x, y))

    def __ne__(self, other):
        return self._op2(other, lambda x, y: x != y, lambda x, y: x != y)

    def __lt__(self, other):
        return self._op2(other, lambda x, y: x &lt; y, lambda x, y: x &lt; y)

    def __le__(self, other):
        return self._op2(other, lambda x, y: x &lt;= y, lambda x, y: x &lt;= y)

    def __gt__(self, other):
        return self._op2(other, lambda x, y: x &gt; y, lambda x, y: x &gt; y)

    def __ge__(self, other):
        return self._op2(other, lambda x, y: x &gt;= y, lambda x, y: x &gt;= y)

    def __abs__(self):
        return self._op1(lambda t: choose_backend(t).abs(t))

    def as_complex(self):
        return self._op1(lambda t: choose_backend(t).to_complex(t))

    def as_float(self):
        return self._op1(lambda t: choose_backend(t).to_float(t))

    def as_int(self, int64=False):
        return self._op1(lambda t: choose_backend(t).to_int(t, int64=int64))

    def __copy__(self):
        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=True))

    def __deepcopy__(self, memodict={}):
        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=False))

    def __neg__(self):
        return self._op1(lambda t: -t)

    def __reversed__(self):
        assert self.shape.channel.rank == 1
        return self[::-1]

    def __iter__(self):
        assert self.rank == 1, f&#34;Can only iterate over 1D tensors but got {self.shape}&#34;
        return iter(self.native())

    def _tensor(self, other):
        if isinstance(other, Tensor):
            return other
        elif isinstance(other, Shape):
            assert self.shape.channel.rank == 1, &#34;Only single-channel tensors support implicit casting from Shape to tensor&#34;
            assert other.rank == self.shape.channel.volume
            return tensor(other.spatial.sizes, names=self.shape.channel.names)
        else:
            backend = choose_backend(other)
            try:
                other_tensor = backend.as_tensor(other, convert_external=True)
                shape = backend.staticshape(other_tensor)
            except ValueError as e:
                raise ValueError(e)
            if len(shape) == 0:
                return NativeTensor(other_tensor, EMPTY_SHAPE)
            elif len(shape) == self.rank:
                return NativeTensor(other_tensor, self.shape.with_sizes(shape))
            elif len(shape) == self.shape.channel.rank:
                other_tensor = tensor(other, names=self.shape.channel.names)
                return other_tensor
            elif len(shape) == 1 and self.shape.channel.rank == 0:
                return NativeTensor(other_tensor, Shape(shape, [&#39;vector&#39;], [CHANNEL_DIM]))
            else:
                raise ValueError(&#34;Cannot broadcast object of rank %d to tensor with shape %s&#34; % (backend.ndims(other), self.shape))

    def _op1(self, native_function):
        &#34;&#34;&#34;
        Transform the values of this tensor given a function that can be applied to any native tensor.

        Args:
          native_function:

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def _op2(self, other: &#39;Tensor&#39;, operator: callable, native_function: callable) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Apply a broadcast operation on two tensors.

        Args:
          other: second argument
          operator: function (Tensor, Tensor) -&gt; Tensor, used to propagate the operation to children tensors to have Python choose the callee
          native_function: function (native tensor, native tensor) -&gt; native tensor
          other: &#39;Tensor&#39;: 
          operator: callable: 
          native_function: callable: 

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError()

    def _natives(self) -&gt; tuple:
        raise NotImplementedError(self.__class__)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li>phi.math._tensors.CollapsedTensor</li>
<li>phi.math._tensors.NativeTensor</li>
<li>phi.math._tensors.TensorStack</li>
<li>phi.math._track.ShiftLinOp</li>
<li>phi.math._track.SparseLinearOperation</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Tensor.dtype"><code class="name">var <span class="ident">dtype</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dtype(self):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.ndims"><code class="name">var <span class="ident">ndims</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def ndims(self) -&gt; int:
    return self.shape.rank</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.rank"><code class="name">var <span class="ident">rank</span> : int</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rank(self) -&gt; int:
    return self.shape.rank</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.shape"><code class="name">var <span class="ident">shape</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self) -&gt; Shape:
    raise NotImplementedError()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Tensor.as_complex"><code class="name flex">
<span>def <span class="ident">as_complex</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def as_complex(self):
    return self._op1(lambda t: choose_backend(t).to_complex(t))</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.as_float"><code class="name flex">
<span>def <span class="ident">as_float</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def as_float(self):
    return self._op1(lambda t: choose_backend(t).to_float(t))</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.as_int"><code class="name flex">
<span>def <span class="ident">as_int</span></span>(<span>self, int64=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def as_int(self, int64=False):
    return self._op1(lambda t: choose_backend(t).to_int(t, int64=int64))</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.dimension"><code class="name flex">
<span>def <span class="ident">dimension</span></span>(<span>self, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dimension(self, name):
    return _TensorDim(self, name)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.flip"><code class="name flex">
<span>def <span class="ident">flip</span></span>(<span>self, *dims: str) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flip(self, *dims: str) -&gt; &#39;Tensor&#39;:
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.native"><code class="name flex">
<span>def <span class="ident">native</span></span>(<span>self, order: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a native tensor object with the dimensions ordered according to <code>order</code>.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.</p>
<p>If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>order</code></strong></dt>
<dd>optional) list of dimension names. If not given, the current order is kept.</dd>
<dt><strong><code>order</code></strong></dt>
<dd>str or tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>native tensor object
:raise: ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def native(self, order: str or tuple or list = None):
    &#34;&#34;&#34;
    Returns a native tensor object with the dimensions ordered according to `order`.
    
    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
    
    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

    Args:
      order: optional) list of dimension names. If not given, the current order is kept.
      order: str or tuple or list:  (Default value = None)

    Returns:
      native tensor object
      :raise: ValueError if the tensor cannot be transposed to match target_shape

    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.numpy"><code class="name flex">
<span>def <span class="ident">numpy</span></span>(<span>self, order: str = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns this tensor as a NumPy ndarray object with dimensions ordered according to <code>order</code>.</p>
<p><em>Note</em>: Using this function breaks the autograd chain. The returned tensor is not differentiable.
To get a differentiable tensor, use :func:<code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">Tensor.native()</a></code> instead.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.</p>
<p>If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>order</code></strong></dt>
<dd>optional) list of dimension names. If not given, the current order is kept.</dd>
<dt><strong><code>order</code></strong></dt>
<dd>str or tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>NumPy representation
:raise: ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def numpy(self, order: str or tuple or list = None) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Returns this tensor as a NumPy ndarray object with dimensions ordered according to `order`.
    
    *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
    To get a differentiable tensor, use :func:`Tensor.native` instead.
    
    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
    
    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

    Args:
      order: optional) list of dimension names. If not given, the current order is kept.
      order: str or tuple or list:  (Default value = None)

    Returns:
      NumPy representation
      :raise: ValueError if the tensor cannot be transposed to match target_shape

    &#34;&#34;&#34;
    native = self.native(order=order)
    return choose_backend(native).numpy(native)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, dimension)</span>
</code></dt>
<dd>
<div class="desc"><p>Splits this tensor along the specified dimension.
The returned tensors have the same dimensions as this tensor save the unstacked dimension.</p>
<h2 id="args">Args</h2>
<p>dimension(str or int or _TensorDim): name of dimension or Dimension or None for component dimension</p>
<h2 id="returns">Returns</h2>
<p>tuple of tensors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(self, dimension):
    &#34;&#34;&#34;
    Splits this tensor along the specified dimension.
    The returned tensors have the same dimensions as this tensor save the unstacked dimension.

    Args:
      dimension(str or int or _TensorDim): name of dimension or Dimension or None for component dimension

    Returns:
      tuple of tensors

    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="phi" href="../index.html">phi</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code></li>
<li><code><a title="phi.math.extrapolation" href="extrapolation.html">phi.math.extrapolation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="phi.math.PI" href="#phi.math.PI">PI</a></code></li>
<li><code><a title="phi.math.SCIPY_BACKEND" href="#phi.math.SCIPY_BACKEND">SCIPY_BACKEND</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="phi.math.abs" href="#phi.math.abs">abs</a></code></li>
<li><code><a title="phi.math.abs_square" href="#phi.math.abs_square">abs_square</a></code></li>
<li><code><a title="phi.math.all" href="#phi.math.all">all</a></code></li>
<li><code><a title="phi.math.all_available" href="#phi.math.all_available">all_available</a></code></li>
<li><code><a title="phi.math.any" href="#phi.math.any">any</a></code></li>
<li><code><a title="phi.math.assert_close" href="#phi.math.assert_close">assert_close</a></code></li>
<li><code><a title="phi.math.batch_shape" href="#phi.math.batch_shape">batch_shape</a></code></li>
<li><code><a title="phi.math.batch_stack" href="#phi.math.batch_stack">batch_stack</a></code></li>
<li><code><a title="phi.math.boolean_mask" href="#phi.math.boolean_mask">boolean_mask</a></code></li>
<li><code><a title="phi.math.cast" href="#phi.math.cast">cast</a></code></li>
<li><code><a title="phi.math.ceil" href="#phi.math.ceil">ceil</a></code></li>
<li><code><a title="phi.math.channel_shape" href="#phi.math.channel_shape">channel_shape</a></code></li>
<li><code><a title="phi.math.channel_stack" href="#phi.math.channel_stack">channel_stack</a></code></li>
<li><code><a title="phi.math.choose_backend" href="#phi.math.choose_backend">choose_backend</a></code></li>
<li><code><a title="phi.math.clip" href="#phi.math.clip">clip</a></code></li>
<li><code><a title="phi.math.close" href="#phi.math.close">close</a></code></li>
<li><code><a title="phi.math.closest_grid_values" href="#phi.math.closest_grid_values">closest_grid_values</a></code></li>
<li><code><a title="phi.math.concat" href="#phi.math.concat">concat</a></code></li>
<li><code><a title="phi.math.conv" href="#phi.math.conv">conv</a></code></li>
<li><code><a title="phi.math.cos" href="#phi.math.cos">cos</a></code></li>
<li><code><a title="phi.math.cross_product" href="#phi.math.cross_product">cross_product</a></code></li>
<li><code><a title="phi.math.divide_no_nan" href="#phi.math.divide_no_nan">divide_no_nan</a></code></li>
<li><code><a title="phi.math.dot" href="#phi.math.dot">dot</a></code></li>
<li><code><a title="phi.math.downsample2x" href="#phi.math.downsample2x">downsample2x</a></code></li>
<li><code><a title="phi.math.dtype" href="#phi.math.dtype">dtype</a></code></li>
<li><code><a title="phi.math.einsum" href="#phi.math.einsum">einsum</a></code></li>
<li><code><a title="phi.math.exp" href="#phi.math.exp">exp</a></code></li>
<li><code><a title="phi.math.expand_channel" href="#phi.math.expand_channel">expand_channel</a></code></li>
<li><code><a title="phi.math.fft" href="#phi.math.fft">fft</a></code></li>
<li><code><a title="phi.math.fftfreq" href="#phi.math.fftfreq">fftfreq</a></code></li>
<li><code><a title="phi.math.floor" href="#phi.math.floor">floor</a></code></li>
<li><code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace</a></code></li>
<li><code><a title="phi.math.fourier_poisson" href="#phi.math.fourier_poisson">fourier_poisson</a></code></li>
<li><code><a title="phi.math.frequency_loss" href="#phi.math.frequency_loss">frequency_loss</a></code></li>
<li><code><a title="phi.math.get_precision" href="#phi.math.get_precision">get_precision</a></code></li>
<li><code><a title="phi.math.gradient" href="#phi.math.gradient">gradient</a></code></li>
<li><code><a title="phi.math.grid_sample" href="#phi.math.grid_sample">grid_sample</a></code></li>
<li><code><a title="phi.math.ifft" href="#phi.math.ifft">ifft</a></code></li>
<li><code><a title="phi.math.imag" href="#phi.math.imag">imag</a></code></li>
<li><code><a title="phi.math.isfinite" href="#phi.math.isfinite">isfinite</a></code></li>
<li><code><a title="phi.math.join_dimensions" href="#phi.math.join_dimensions">join_dimensions</a></code></li>
<li><code><a title="phi.math.l1_loss" href="#phi.math.l1_loss">l1_loss</a></code></li>
<li><code><a title="phi.math.l2_loss" href="#phi.math.l2_loss">l2_loss</a></code></li>
<li><code><a title="phi.math.l_n_loss" href="#phi.math.l_n_loss">l_n_loss</a></code></li>
<li><code><a title="phi.math.laplace" href="#phi.math.laplace">laplace</a></code></li>
<li><code><a title="phi.math.linspace" href="#phi.math.linspace">linspace</a></code></li>
<li><code><a title="phi.math.matmul" href="#phi.math.matmul">matmul</a></code></li>
<li><code><a title="phi.math.max" href="#phi.math.max">max</a></code></li>
<li><code><a title="phi.math.maximum" href="#phi.math.maximum">maximum</a></code></li>
<li><code><a title="phi.math.mean" href="#phi.math.mean">mean</a></code></li>
<li><code><a title="phi.math.meshgrid" href="#phi.math.meshgrid">meshgrid</a></code></li>
<li><code><a title="phi.math.min" href="#phi.math.min">min</a></code></li>
<li><code><a title="phi.math.minimum" href="#phi.math.minimum">minimum</a></code></li>
<li><code><a title="phi.math.nonzero" href="#phi.math.nonzero">nonzero</a></code></li>
<li><code><a title="phi.math.normalize_to" href="#phi.math.normalize_to">normalize_to</a></code></li>
<li><code><a title="phi.math.ones" href="#phi.math.ones">ones</a></code></li>
<li><code><a title="phi.math.ones_like" href="#phi.math.ones_like">ones_like</a></code></li>
<li><code><a title="phi.math.pad" href="#phi.math.pad">pad</a></code></li>
<li><code><a title="phi.math.precision" href="#phi.math.precision">precision</a></code></li>
<li><code><a title="phi.math.print" href="#phi.math.print">print</a></code></li>
<li><code><a title="phi.math.prod" href="#phi.math.prod">prod</a></code></li>
<li><code><a title="phi.math.random_normal" href="#phi.math.random_normal">random_normal</a></code></li>
<li><code><a title="phi.math.random_uniform" href="#phi.math.random_uniform">random_uniform</a></code></li>
<li><code><a title="phi.math.real" href="#phi.math.real">real</a></code></li>
<li><code><a title="phi.math.round" href="#phi.math.round">round</a></code></li>
<li><code><a title="phi.math.sample_subgrid" href="#phi.math.sample_subgrid">sample_subgrid</a></code></li>
<li><code><a title="phi.math.scatter" href="#phi.math.scatter">scatter</a></code></li>
<li><code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision</a></code></li>
<li><code><a title="phi.math.shape" href="#phi.math.shape">shape</a></code></li>
<li><code><a title="phi.math.shift" href="#phi.math.shift">shift</a></code></li>
<li><code><a title="phi.math.sign" href="#phi.math.sign">sign</a></code></li>
<li><code><a title="phi.math.sin" href="#phi.math.sin">sin</a></code></li>
<li><code><a title="phi.math.solve" href="#phi.math.solve">solve</a></code></li>
<li><code><a title="phi.math.sparse_tensor" href="#phi.math.sparse_tensor">sparse_tensor</a></code></li>
<li><code><a title="phi.math.spatial_pad" href="#phi.math.spatial_pad">spatial_pad</a></code></li>
<li><code><a title="phi.math.spatial_shape" href="#phi.math.spatial_shape">spatial_shape</a></code></li>
<li><code><a title="phi.math.spatial_stack" href="#phi.math.spatial_stack">spatial_stack</a></code></li>
<li><code><a title="phi.math.spatial_sum" href="#phi.math.spatial_sum">spatial_sum</a></code></li>
<li><code><a title="phi.math.sqrt" href="#phi.math.sqrt">sqrt</a></code></li>
<li><code><a title="phi.math.std" href="#phi.math.std">std</a></code></li>
<li><code><a title="phi.math.sum" href="#phi.math.sum">sum</a></code></li>
<li><code><a title="phi.math.tensor" href="#phi.math.tensor">tensor</a></code></li>
<li><code><a title="phi.math.tensors" href="#phi.math.tensors">tensors</a></code></li>
<li><code><a title="phi.math.tile" href="#phi.math.tile">tile</a></code></li>
<li><code><a title="phi.math.to_complex" href="#phi.math.to_complex">to_complex</a></code></li>
<li><code><a title="phi.math.to_float" href="#phi.math.to_float">to_float</a></code></li>
<li><code><a title="phi.math.to_int" href="#phi.math.to_int">to_int</a></code></li>
<li><code><a title="phi.math.transpose" href="#phi.math.transpose">transpose</a></code></li>
<li><code><a title="phi.math.unstack" href="#phi.math.unstack">unstack</a></code></li>
<li><code><a title="phi.math.upsample2x" href="#phi.math.upsample2x">upsample2x</a></code></li>
<li><code><a title="phi.math.vec_abs" href="#phi.math.vec_abs">vec_abs</a></code></li>
<li><code><a title="phi.math.vec_squared" href="#phi.math.vec_squared">vec_squared</a></code></li>
<li><code><a title="phi.math.where" href="#phi.math.where">where</a></code></li>
<li><code><a title="phi.math.with_custom_gradient" href="#phi.math.with_custom_gradient">with_custom_gradient</a></code></li>
<li><code><a title="phi.math.zeros" href="#phi.math.zeros">zeros</a></code></li>
<li><code><a title="phi.math.zeros_like" href="#phi.math.zeros_like">zeros_like</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code></h4>
<ul class="">
<li><code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">bits</a></code></li>
<li><code><a title="phi.math.DType.itemsize" href="#phi.math.DType.itemsize">itemsize</a></code></li>
<li><code><a title="phi.math.DType.kind" href="#phi.math.DType.kind">kind</a></code></li>
<li><code><a title="phi.math.DType.precision" href="#phi.math.DType.precision">precision</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.Extrapolation" href="#phi.math.Extrapolation">Extrapolation</a></code></h4>
<ul class="">
<li><code><a title="phi.math.Extrapolation.gradient" href="#phi.math.Extrapolation.gradient">gradient</a></code></li>
<li><code><a title="phi.math.Extrapolation.is_copy_pad" href="#phi.math.Extrapolation.is_copy_pad">is_copy_pad</a></code></li>
<li><code><a title="phi.math.Extrapolation.native_grid_sample_mode" href="#phi.math.Extrapolation.native_grid_sample_mode">native_grid_sample_mode</a></code></li>
<li><code><a title="phi.math.Extrapolation.pad" href="#phi.math.Extrapolation.pad">pad</a></code></li>
<li><code><a title="phi.math.Extrapolation.pad_values" href="#phi.math.Extrapolation.pad_values">pad_values</a></code></li>
<li><code><a title="phi.math.Extrapolation.to_dict" href="#phi.math.Extrapolation.to_dict">to_dict</a></code></li>
<li><code><a title="phi.math.Extrapolation.transform_coordinates" href="#phi.math.Extrapolation.transform_coordinates">transform_coordinates</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.LinearSolve" href="#phi.math.LinearSolve">LinearSolve</a></code></h4>
</li>
<li>
<h4><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.Shape.after_gather" href="#phi.math.Shape.after_gather">after_gather</a></code></li>
<li><code><a title="phi.math.Shape.after_pad" href="#phi.math.Shape.after_pad">after_pad</a></code></li>
<li><code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">batch</a></code></li>
<li><code><a title="phi.math.Shape.batch_rank" href="#phi.math.Shape.batch_rank">batch_rank</a></code></li>
<li><code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">channel</a></code></li>
<li><code><a title="phi.math.Shape.channel_rank" href="#phi.math.Shape.channel_rank">channel_rank</a></code></li>
<li><code><a title="phi.math.Shape.combined" href="#phi.math.Shape.combined">combined</a></code></li>
<li><code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">defined</a></code></li>
<li><code><a title="phi.math.Shape.dimensions" href="#phi.math.Shape.dimensions">dimensions</a></code></li>
<li><code><a title="phi.math.Shape.expand" href="#phi.math.Shape.expand">expand</a></code></li>
<li><code><a title="phi.math.Shape.expand_batch" href="#phi.math.Shape.expand_batch">expand_batch</a></code></li>
<li><code><a title="phi.math.Shape.expand_channel" href="#phi.math.Shape.expand_channel">expand_channel</a></code></li>
<li><code><a title="phi.math.Shape.expand_spatial" href="#phi.math.Shape.expand_spatial">expand_spatial</a></code></li>
<li><code><a title="phi.math.Shape.extend" href="#phi.math.Shape.extend">extend</a></code></li>
<li><code><a title="phi.math.Shape.get_size" href="#phi.math.Shape.get_size">get_size</a></code></li>
<li><code><a title="phi.math.Shape.get_type" href="#phi.math.Shape.get_type">get_type</a></code></li>
<li><code><a title="phi.math.Shape.index" href="#phi.math.Shape.index">index</a></code></li>
<li><code><a title="phi.math.Shape.indices" href="#phi.math.Shape.indices">indices</a></code></li>
<li><code><a title="phi.math.Shape.is_batch" href="#phi.math.Shape.is_batch">is_batch</a></code></li>
<li><code><a title="phi.math.Shape.is_channel" href="#phi.math.Shape.is_channel">is_channel</a></code></li>
<li><code><a title="phi.math.Shape.is_empty" href="#phi.math.Shape.is_empty">is_empty</a></code></li>
<li><code><a title="phi.math.Shape.is_non_uniform" href="#phi.math.Shape.is_non_uniform">is_non_uniform</a></code></li>
<li><code><a title="phi.math.Shape.is_spatial" href="#phi.math.Shape.is_spatial">is_spatial</a></code></li>
<li><code><a title="phi.math.Shape.mask" href="#phi.math.Shape.mask">mask</a></code></li>
<li><code><a title="phi.math.Shape.meshgrid" href="#phi.math.Shape.meshgrid">meshgrid</a></code></li>
<li><code><a title="phi.math.Shape.name" href="#phi.math.Shape.name">name</a></code></li>
<li><code><a title="phi.math.Shape.named_sizes" href="#phi.math.Shape.named_sizes">named_sizes</a></code></li>
<li><code><a title="phi.math.Shape.names" href="#phi.math.Shape.names">names</a></code></li>
<li><code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">non_batch</a></code></li>
<li><code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">non_channel</a></code></li>
<li><code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">non_singleton</a></code></li>
<li><code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">non_spatial</a></code></li>
<li><code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">non_zero</a></code></li>
<li><code><a title="phi.math.Shape.normal_order" href="#phi.math.Shape.normal_order">normal_order</a></code></li>
<li><code><a title="phi.math.Shape.only" href="#phi.math.Shape.only">only</a></code></li>
<li><code><a title="phi.math.Shape.order" href="#phi.math.Shape.order">order</a></code></li>
<li><code><a title="phi.math.Shape.order_group" href="#phi.math.Shape.order_group">order_group</a></code></li>
<li><code><a title="phi.math.Shape.perm" href="#phi.math.Shape.perm">perm</a></code></li>
<li><code><a title="phi.math.Shape.product" href="#phi.math.Shape.product">product</a></code></li>
<li><code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">rank</a></code></li>
<li><code><a title="phi.math.Shape.reduce" href="#phi.math.Shape.reduce">reduce</a></code></li>
<li><code><a title="phi.math.Shape.reorder" href="#phi.math.Shape.reorder">reorder</a></code></li>
<li><code><a title="phi.math.Shape.select" href="#phi.math.Shape.select">select</a></code></li>
<li><code><a title="phi.math.Shape.sequence_get" href="#phi.math.Shape.sequence_get">sequence_get</a></code></li>
<li><code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">shape</a></code></li>
<li><code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">singleton</a></code></li>
<li><code><a title="phi.math.Shape.sizes" href="#phi.math.Shape.sizes">sizes</a></code></li>
<li><code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">spatial</a></code></li>
<li><code><a title="phi.math.Shape.spatial_dict" href="#phi.math.Shape.spatial_dict">spatial_dict</a></code></li>
<li><code><a title="phi.math.Shape.spatial_rank" href="#phi.math.Shape.spatial_rank">spatial_rank</a></code></li>
<li><code><a title="phi.math.Shape.to_batch" href="#phi.math.Shape.to_batch">to_batch</a></code></li>
<li><code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">undefined</a></code></li>
<li><code><a title="phi.math.Shape.unstack" href="#phi.math.Shape.unstack">unstack</a></code></li>
<li><code><a title="phi.math.Shape.volume" href="#phi.math.Shape.volume">volume</a></code></li>
<li><code><a title="phi.math.Shape.well_defined" href="#phi.math.Shape.well_defined">well_defined</a></code></li>
<li><code><a title="phi.math.Shape.with_names" href="#phi.math.Shape.with_names">with_names</a></code></li>
<li><code><a title="phi.math.Shape.with_size" href="#phi.math.Shape.with_size">with_size</a></code></li>
<li><code><a title="phi.math.Shape.with_sizes" href="#phi.math.Shape.with_sizes">with_sizes</a></code></li>
<li><code><a title="phi.math.Shape.with_types" href="#phi.math.Shape.with_types">with_types</a></code></li>
<li><code><a title="phi.math.Shape.without" href="#phi.math.Shape.without">without</a></code></li>
<li><code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">zero</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code></h4>
</li>
<li>
<h4><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.Tensor.as_complex" href="#phi.math.Tensor.as_complex">as_complex</a></code></li>
<li><code><a title="phi.math.Tensor.as_float" href="#phi.math.Tensor.as_float">as_float</a></code></li>
<li><code><a title="phi.math.Tensor.as_int" href="#phi.math.Tensor.as_int">as_int</a></code></li>
<li><code><a title="phi.math.Tensor.dimension" href="#phi.math.Tensor.dimension">dimension</a></code></li>
<li><code><a title="phi.math.Tensor.dtype" href="#phi.math.Tensor.dtype">dtype</a></code></li>
<li><code><a title="phi.math.Tensor.flip" href="#phi.math.Tensor.flip">flip</a></code></li>
<li><code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">native</a></code></li>
<li><code><a title="phi.math.Tensor.ndims" href="#phi.math.Tensor.ndims">ndims</a></code></li>
<li><code><a title="phi.math.Tensor.numpy" href="#phi.math.Tensor.numpy">numpy</a></code></li>
<li><code><a title="phi.math.Tensor.rank" href="#phi.math.Tensor.rank">rank</a></code></li>
<li><code><a title="phi.math.Tensor.shape" href="#phi.math.Tensor.shape">shape</a></code></li>
<li><code><a title="phi.math.Tensor.unstack" href="#phi.math.Tensor.unstack">unstack</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>